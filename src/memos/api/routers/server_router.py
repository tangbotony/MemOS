import json
import os
import traceback

from typing import TYPE_CHECKING, Any
import re

from fastapi import APIRouter, HTTPException

from memos.api.config import APIConfig
from memos.api.product_models import (
    APIADDRequest,
    APIChatCompleteRequest,
    APISearchRequest,
    MemoryResponse,
    SearchResponse,
)
from memos.configs.embedder import EmbedderConfigFactory
from memos.configs.graph_db import GraphDBConfigFactory
from memos.configs.internet_retriever import InternetRetrieverConfigFactory
from memos.configs.llm import LLMConfigFactory
from memos.configs.mem_reader import MemReaderConfigFactory
from memos.configs.mem_scheduler import SchedulerConfigFactory
from memos.configs.reranker import RerankerConfigFactory
from memos.configs.vec_db import VectorDBConfigFactory
from memos.context.context import ContextThreadPoolExecutor
from memos.embedders.factory import EmbedderFactory
from memos.graph_dbs.factory import GraphStoreFactory
from memos.llms.factory import LLMFactory
from memos.log import get_logger
from memos.mem_cube.navie import NaiveMemCube
from memos.mem_os.product_server import MOSServer
from memos.mem_reader.factory import MemReaderFactory
from memos.mem_scheduler.orm_modules.base_model import BaseDBManager
from memos.mem_scheduler.scheduler_factory import SchedulerFactory
from memos.mem_scheduler.schemas.general_schemas import (
    SearchMode,
)
from memos.memories.textual.prefer_text_memory.config import (
    AdderConfigFactory,
    ExtractorConfigFactory,
    RetrieverConfigFactory,
)
from memos.memories.textual.prefer_text_memory.factory import (
    AdderFactory,
    ExtractorFactory,
    RetrieverFactory,
)
from memos.memories.textual.tree_text_memory.organize.manager import MemoryManager
from memos.memories.textual.tree_text_memory.retrieve.internet_retriever_factory import (
    InternetRetrieverFactory,
)
from memos.reranker.factory import RerankerFactory
from memos.templates.instruction_completion import instruct_completion


if TYPE_CHECKING:
    from memos.mem_scheduler.optimized_scheduler import OptimizedScheduler
from memos.types import MOSSearchResult, UserContext
from memos.vec_dbs.factory import VecDBFactory


logger = get_logger(__name__)

router = APIRouter(prefix="/product", tags=["Server API"])


def _build_graph_db_config(user_id: str = "default") -> dict[str, Any]:
    """Build graph database configuration."""
    graph_db_backend_map = {
        "neo4j-community": APIConfig.get_neo4j_community_config(user_id=user_id),
        "neo4j": APIConfig.get_neo4j_config(user_id=user_id),
        "nebular": APIConfig.get_nebular_config(user_id=user_id),
        "polardb": APIConfig.get_polardb_config(user_id=user_id),
    }

    graph_db_backend = os.getenv("NEO4J_BACKEND", "nebular").lower()
    return GraphDBConfigFactory.model_validate(
        {
            "backend": graph_db_backend,
            "config": graph_db_backend_map[graph_db_backend],
        }
    )


def _build_vec_db_config() -> dict[str, Any]:
    """Build vector database configuration."""
    return VectorDBConfigFactory.model_validate(
        {
            "backend": "milvus",
            "config": APIConfig.get_milvus_config(),
        }
    )


def _build_llm_config() -> dict[str, Any]:
    """Build LLM configuration."""
    return LLMConfigFactory.model_validate(
        {
            "backend": "openai",
            "config": APIConfig.get_openai_config(),
        }
    )


def _build_embedder_config() -> dict[str, Any]:
    """Build embedder configuration."""
    return EmbedderConfigFactory.model_validate(APIConfig.get_embedder_config())


def _build_mem_reader_config() -> dict[str, Any]:
    """Build memory reader configuration."""
    return MemReaderConfigFactory.model_validate(
        APIConfig.get_product_default_config()["mem_reader"]
    )


def _build_reranker_config() -> dict[str, Any]:
    """Build reranker configuration."""
    return RerankerConfigFactory.model_validate(APIConfig.get_reranker_config())


def _build_internet_retriever_config() -> dict[str, Any]:
    """Build internet retriever configuration."""
    return InternetRetrieverConfigFactory.model_validate(APIConfig.get_internet_config())


def _build_pref_extractor_config() -> dict[str, Any]:
    """Build extractor configuration."""
    return ExtractorConfigFactory.model_validate({"backend": "naive", "config": {}})


def _build_pref_adder_config() -> dict[str, Any]:
    """Build adder configuration."""
    return AdderConfigFactory.model_validate({"backend": "naive", "config": {}})


def _build_pref_retriever_config() -> dict[str, Any]:
    """Build retriever configuration."""
    return RetrieverConfigFactory.model_validate({"backend": "naive", "config": {}})


def _get_default_memory_size(cube_config) -> dict[str, int]:
    """Get default memory size configuration."""
    return getattr(cube_config.text_mem.config, "memory_size", None) or {
        "WorkingMemory": 20,
        "LongTermMemory": 1500,
        "UserMemory": 480,
    }


def init_server():
    """Initialize server components and configurations."""
    # Get default cube configuration
    default_cube_config = APIConfig.get_default_cube_config()

    # Build component configurations
    graph_db_config = _build_graph_db_config()
    llm_config = _build_llm_config()
    embedder_config = _build_embedder_config()
    mem_reader_config = _build_mem_reader_config()
    reranker_config = _build_reranker_config()
    internet_retriever_config = _build_internet_retriever_config()
    vector_db_config = _build_vec_db_config()
    pref_extractor_config = _build_pref_extractor_config()
    pref_adder_config = _build_pref_adder_config()
    pref_retriever_config = _build_pref_retriever_config()

    # Create component instances
    graph_db = GraphStoreFactory.from_config(graph_db_config)
    vector_db = VecDBFactory.from_config(vector_db_config)
    llm = LLMFactory.from_config(llm_config)
    embedder = EmbedderFactory.from_config(embedder_config)
    mem_reader = MemReaderFactory.from_config(mem_reader_config)
    reranker = RerankerFactory.from_config(reranker_config)
    internet_retriever = InternetRetrieverFactory.from_config(
        internet_retriever_config, embedder=embedder
    )
    pref_extractor = ExtractorFactory.from_config(
        config_factory=pref_extractor_config,
        llm_provider=llm,
        embedder=embedder,
        vector_db=vector_db,
    )
    pref_adder = AdderFactory.from_config(
        config_factory=pref_adder_config,
        llm_provider=llm,
        embedder=embedder,
        vector_db=vector_db,
    )
    pref_retriever = RetrieverFactory.from_config(
        config_factory=pref_retriever_config,
        llm_provider=llm,
        embedder=embedder,
        reranker=reranker,
        vector_db=vector_db,
    )

    # Initialize memory manager
    memory_manager = MemoryManager(
        graph_db,
        embedder,
        llm,
        memory_size=_get_default_memory_size(default_cube_config),
        is_reorganize=getattr(default_cube_config.text_mem.config, "reorganize", False),
    )
    mos_server = MOSServer(
        mem_reader=mem_reader,
        llm=llm,
        online_bot=False,
    )

    naive_mem_cube = NaiveMemCube(
        llm=llm,
        embedder=embedder,
        mem_reader=mem_reader,
        graph_db=graph_db,
        reranker=reranker,
        internet_retriever=internet_retriever,
        memory_manager=memory_manager,
        default_cube_config=default_cube_config,
        vector_db=vector_db,
        pref_extractor=pref_extractor,
        pref_adder=pref_adder,
        pref_retriever=pref_retriever,
    )

    # Initialize Scheduler
    scheduler_config_dict = APIConfig.get_scheduler_config()
    scheduler_config = SchedulerConfigFactory(
        backend="optimized_scheduler", config=scheduler_config_dict
    )
    mem_scheduler: OptimizedScheduler = SchedulerFactory.from_config(scheduler_config)
    mem_scheduler.initialize_modules(
        chat_llm=llm,
        process_llm=mem_reader.llm,
        db_engine=BaseDBManager.create_default_sqlite_engine(),
    )
    mem_scheduler.current_mem_cube = naive_mem_cube
    mem_scheduler.start()

    # Initialize SchedulerAPIModule
    api_module = mem_scheduler.api_module

    return (
        graph_db,
        mem_reader,
        llm,
        embedder,
        reranker,
        internet_retriever,
        memory_manager,
        default_cube_config,
        mos_server,
        mem_scheduler,
        naive_mem_cube,
        api_module,
        vector_db,
        pref_extractor,
        pref_adder,
        pref_retriever,
    )


# Initialize global components
(
    graph_db,
    mem_reader,
    llm,
    embedder,
    reranker,
    internet_retriever,
    memory_manager,
    default_cube_config,
    mos_server,
    mem_scheduler,
    naive_mem_cube,
    api_module,
    vector_db,
    pref_extractor,
    pref_adder,
    pref_retriever,
) = init_server()


def _format_memory_item(memory_data: Any) -> dict[str, Any]:
    """Format a single memory item for API response."""
    memory = memory_data.model_dump()
    memory_id = memory["id"]
    ref_id = f"[{memory_id.split('-')[0]}]"

    memory["ref_id"] = ref_id
    memory["metadata"]["embedding"] = []
    memory["metadata"]["sources"] = []
    memory["metadata"]["ref_id"] = ref_id
    memory["metadata"]["id"] = memory_id
    memory["metadata"]["memory"] = memory["memory"]

    return memory


def _post_process_pref_mem(
    memories_result: list[dict[str, Any]],
    pref_formatted_mem: list[dict[str, Any]],
    mem_cube_id: str,
    handle_pref_mem: bool,
):
    if handle_pref_mem:
        memories_result["pref_mem"].append(
            {
                "cube_id": mem_cube_id,
                "memories": pref_formatted_mem,
            }
        )
        pref_instruction: str = instruct_completion(pref_formatted_mem)
        memories_result["pref_string"] = pref_instruction

    return memories_result


@router.post("/search", summary="Search memories", response_model=SearchResponse)
def search_memories(search_req: APISearchRequest):
    """Search memories for a specific user."""
    # Create UserContext object - how to assign values
    user_context = UserContext(
        user_id=search_req.user_id,
        mem_cube_id=search_req.mem_cube_id,
        session_id=search_req.session_id or "default_session",
    )
    logger.info(f"Search user_id is: {user_context.mem_cube_id}")
    memories_result: MOSSearchResult = {
        "text_mem": [],
        "act_mem": [],
        "para_mem": [],
        "pref_mem": [],
        "pref_string": "",
    }

    search_mode = search_req.mode

    def _search_text():
        if search_mode == SearchMode.FAST:
            formatted_memories = fast_search_memories(
                search_req=search_req, user_context=user_context
            )
        elif search_mode == SearchMode.FINE:
            formatted_memories = fine_search_memories(
                search_req=search_req, user_context=user_context
            )
        elif search_mode == SearchMode.MIXTURE:
            formatted_memories = mix_search_memories(
                search_req=search_req, user_context=user_context
            )
        else:
            logger.error(f"Unsupported search mode: {search_mode}")
            raise HTTPException(status_code=400, detail=f"Unsupported search mode: {search_mode}")
        return formatted_memories

    def _search_pref():
        if os.getenv("ENABLE_PREFERENCE_MEMORY", "false").lower() != "true":
            return []
        results = naive_mem_cube.pref_mem.search(
            query=search_req.query,
            top_k=search_req.top_k,
            info={
                "user_id": search_req.user_id,
                "session_id": search_req.session_id,
                "chat_history": search_req.chat_history,
            },
        )
        return [_format_memory_item(data) for data in results]

    with ContextThreadPoolExecutor(max_workers=2) as executor:
        text_future = executor.submit(_search_text)
        pref_future = executor.submit(_search_pref)
        text_formatted_memories = text_future.result()
        pref_formatted_memories = pref_future.result()

    memories_result["text_mem"].append(
        {
            "cube_id": search_req.mem_cube_id,
            "memories": text_formatted_memories,
        }
    )

    memories_result = _post_process_pref_mem(
        memories_result, pref_formatted_memories, search_req.mem_cube_id, search_req.handle_pref_mem
    )

    return SearchResponse(
        message="Search completed successfully",
        data=memories_result,
    )


def mix_search_memories(
    search_req: APISearchRequest,
    user_context: UserContext,
):
    """
    Mix search memories: fast search + async fine search
    """

    formatted_memories = mem_scheduler.mix_search_memories(
        search_req=search_req,
        user_context=user_context,
    )
    return formatted_memories


def fine_search_memories(
    search_req: APISearchRequest,
    user_context: UserContext,
):
    target_session_id = search_req.session_id
    if not target_session_id:
        target_session_id = "default_session"
    search_filter = {"session_id": search_req.session_id} if search_req.session_id else None

    # Create MemCube and perform search
    search_results = naive_mem_cube.text_mem.search(
        query=search_req.query,
        user_name=user_context.mem_cube_id,
        top_k=search_req.top_k,
        mode=SearchMode.FINE,
        manual_close_internet=not search_req.internet_search,
        moscube=search_req.moscube,
        search_filter=search_filter,
        info={
            "user_id": search_req.user_id,
            "session_id": target_session_id,
            "chat_history": search_req.chat_history,
        },
    )
    formatted_memories = [_format_memory_item(data) for data in search_results]

    return formatted_memories


def fast_search_memories(
    search_req: APISearchRequest,
    user_context: UserContext,
):
    target_session_id = search_req.session_id
    if not target_session_id:
        target_session_id = "default_session"
    search_filter = {"session_id": search_req.session_id} if search_req.session_id else None

    # Create MemCube and perform search
    search_results = naive_mem_cube.text_mem.search(
        query=search_req.query,
        user_name=user_context.mem_cube_id,
        top_k=search_req.top_k,
        mode=SearchMode.FAST,
        manual_close_internet=not search_req.internet_search,
        moscube=search_req.moscube,
        search_filter=search_filter,
        info={
            "user_id": search_req.user_id,
            "session_id": target_session_id,
            "chat_history": search_req.chat_history,
        },
    )
    formatted_memories = [_format_memory_item(data) for data in search_results]

    return formatted_memories


def _check_memory_duplication(new_memory: str, existing_memories: list, llm) -> tuple[bool, str]:
    """
    ä½¿ç”¨LLMåˆ¤æ–­æ–°è®°å¿†æ˜¯å¦ä¸å·²æœ‰è®°å¿†é‡å¤
    
    Args:
        new_memory: å¾…æ·»åŠ çš„æ–°è®°å¿†å†…å®¹
        existing_memories: å·²æœ‰çš„è®°å¿†åˆ—è¡¨
        llm: LLMå®ä¾‹
        
    Returns:
        (is_duplicate, reason): æ˜¯å¦é‡å¤åŠåŸå› è¯´æ˜
    """
    if not existing_memories:
        return False, "æ— å·²æœ‰è®°å¿†"
    
    # æ„å»ºå»é‡åˆ¤æ–­çš„Prompt
    existing_memories_text = "\n".join([
        f"{i+1}. {mem.memory}" 
        for i, mem in enumerate(existing_memories[:30])  # å¢åŠ æ¯”å¯¹æ•°é‡ï¼Œé¿å…é—æ¼
    ])
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªè®°å¿†å»é‡ä¸“å®¶ã€‚è¯·**ä¸¥æ ¼åˆ¤æ–­**æ–°è®°å¿†æ˜¯å¦ä¸å·²æœ‰è®°å¿†é‡å¤ã€‚

ã€åˆ¤æ–­åŸåˆ™ - ä»ä¸¥åˆ¤å®šã€‘
âš ï¸ é‡‡ç”¨**ä¿å®ˆç­–ç•¥**ï¼šå½“æ–°æ—§è®°å¿†è¯­ä¹‰åŸºæœ¬ç›¸åŒæ—¶ï¼Œåº”åˆ¤å®šä¸ºé‡å¤

ã€é‡å¤çš„åˆ¤æ–­æ ‡å‡†ï¼ˆæ»¡è¶³ä»»ä¸€å³ä¸ºé‡å¤ï¼‰ã€‘
1. **å®Œå…¨ç›¸åŒ**ï¼šæ–°è®°å¿†ä¸æŸæ¡å·²æœ‰è®°å¿†è¡¨è¾¾çš„æ„æ€å®Œå…¨ä¸€è‡´ï¼ˆå³ä½¿æªè¾ç•¥æœ‰ä¸åŒï¼‰
   - ç¤ºä¾‹ï¼š
     - æ–°ï¼š"ç”·äººå’Œä¸¤ä¸ªå­©å­ç»å¸¸åœ¨æ™šä¸Šä¸è½¦äº’åŠ¨" 
     - æ—§ï¼š"ç”·äººå’Œä¸¤ä¸ªå­©å­ç»å¸¸åœ¨æ™šä¸Šä¸æ±½è½¦äº’åŠ¨"
     - åˆ¤å®šï¼šâœ… é‡å¤ï¼ˆ"è½¦" vs "æ±½è½¦"åªæ˜¯è¡¨è¾¾å·®å¼‚ï¼Œè¯­ä¹‰ç›¸åŒï¼‰

2. **å®è´¨åŒ…å«**ï¼šæ–°è®°å¿†çš„æ ¸å¿ƒä¿¡æ¯å·²ç»è¢«æŸæ¡å·²æœ‰è®°å¿†åŒ…å«
   - ç¤ºä¾‹ï¼š
     - æ–°ï¼š"å®¶åº­æˆå‘˜åœ¨æ™šä¸Šç¦»å¼€"
     - æ—§ï¼š"å®¶åº­æˆå‘˜åœ¨æ™šä¸Š22:00-00:00ä¹‹é—´ç¦»å¼€"
     - åˆ¤å®šï¼šâœ… é‡å¤ï¼ˆæ—§è®°å¿†æ›´è¯¦ç»†ï¼Œæ–°è®°å¿†è¢«åŒ…å«ï¼‰

3. **å¾®å°å·®å¼‚**ï¼šä»…æœ‰å•å¤æ•°ã€å† è¯ï¼ˆa/theï¼‰ã€è½»å¾®æªè¾å·®å¼‚
   - ç¤ºä¾‹ï¼š
     - æ–°ï¼š"A man interacts with cars"
     - æ—§ï¼š"A man interacts with a car"
     - åˆ¤å®šï¼šâœ… é‡å¤ï¼ˆcars vs car æ˜¯å¾®å°å·®å¼‚ï¼‰

ã€ä¸é‡å¤çš„åˆ¤æ–­æ ‡å‡†ï¼ˆéœ€åŒæ—¶æ»¡è¶³ï¼‰ã€‘
âœ“ æ–°è®°å¿†åŒ…å«**å®è´¨æ€§çš„æ–°ä¿¡æ¯**ï¼ˆä¸æ˜¯å¾®å°å·®å¼‚ï¼‰ï¼Œä¾‹å¦‚ï¼š
  - æ–°çš„æ—¶é—´èŒƒå›´ï¼ˆå¦‚ï¼šæ—§è®°å¿†åªæœ‰æ™šä¸Šï¼Œæ–°è®°å¿†å¢åŠ äº†æ—©ä¸Šï¼‰
  - æ–°çš„å¯¹è±¡/ä¸»ä½“ï¼ˆå¦‚ï¼šæ—§è®°å¿†åªæœ‰ç”·äººï¼Œæ–°è®°å¿†å¢åŠ äº†å¥³äººï¼‰
  - æ–°çš„è¡Œä¸ºæ¨¡å¼ï¼ˆå¦‚ï¼šæ—§è®°å¿†æ˜¯"ç¦»å¼€"ï¼Œæ–°è®°å¿†æ˜¯"è¿”å›"ï¼‰
  - æ–°çš„è¯¦ç»†å±æ€§ï¼ˆå¦‚ï¼šæ—§è®°å¿†æ˜¯"è“è‰²è½¦"ï¼Œæ–°è®°å¿†å¢åŠ äº†"é»‘è‰²è½¦"ï¼‰

ã€ç‰¹åˆ«æ³¨æ„ã€‘
- Pattern Memoryï¼ˆè§„å¾‹è®°å¿†ï¼‰ï¼šåªæœ‰å½“æ—¶é—´èŒƒå›´æˆ–å¯¹è±¡æœ‰**æ˜æ˜¾æ‰©å±•**æ—¶æ‰åˆ¤å®šä¸ºä¸é‡å¤
- Inference Memoryï¼ˆæ¨ç†è®°å¿†ï¼‰ï¼šåªæœ‰å½“æ¨ç†ç»“è®ºæœ‰**æœ¬è´¨å˜åŒ–**æ—¶æ‰åˆ¤å®šä¸ºä¸é‡å¤
- è¡¨è¾¾æ–¹å¼çš„ä¼˜åŒ–ã€æªè¾çš„è°ƒæ•´ã€è¯­åºçš„å˜åŒ– â†’ åˆ¤å®šä¸ºé‡å¤
- åŒä¹‰è¯æ›¿æ¢ï¼ˆå¦‚ car/vehicle, cat/felineï¼‰â†’ åˆ¤å®šä¸ºé‡å¤

ã€æ–°è®°å¿†ã€‘
{new_memory}

ã€å·²æœ‰è®°å¿†ï¼ˆç¼–å·æ–¹ä¾¿å¼•ç”¨ï¼‰ã€‘
{existing_memories_text}

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼ˆä¸è¦æœ‰å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
  "is_duplicate": true/false,
  "reason": "åˆ¤æ–­åŸå› ï¼ˆå¦‚æœé‡å¤ï¼Œè¯´æ˜ä¸ç¬¬å‡ æ¡è®°å¿†é‡å¤ï¼›å¦‚æœä¸é‡å¤ï¼Œè¯´æ˜æ–°å¢äº†ä»€ä¹ˆä¿¡æ¯ï¼‰"
}}

ç¤ºä¾‹è¾“å‡ºï¼š
- é‡å¤ï¼š{{"is_duplicate": true, "reason": "ä¸ç¬¬1æ¡è®°å¿†è¯­ä¹‰å®Œå…¨ç›¸åŒï¼Œä»…æªè¾ç•¥æœ‰å·®å¼‚"}}
- ä¸é‡å¤ï¼š{{"is_duplicate": false, "reason": "æ–°å¢äº†æ—©ä¸Šæ—¶é—´æ®µï¼ˆ06:00-10:00ï¼‰ï¼Œæ—§è®°å¿†åªæœ‰æ™šä¸Š"}}
"""
    
    try:
        messages = [{"role": "user", "content": prompt}]
        response = llm.generate(messages)
        
        # å°è¯•è§£æJSONå“åº”
        # æ¸…ç†å“åº”ä¸­çš„markdownä»£ç å—æ ‡è®°
        response_clean = response.strip()
        if response_clean.startswith("```json"):
            response_clean = response_clean[7:]
        if response_clean.startswith("```"):
            response_clean = response_clean[3:]
        if response_clean.endswith("```"):
            response_clean = response_clean[:-3]
        response_clean = response_clean.strip()
        
        result = json.loads(response_clean)
        is_duplicate = result.get("is_duplicate", False)
        reason = result.get("reason", "æ— è¯´æ˜")
        
        return is_duplicate, reason
    except Exception as e:
        logger.warning(f"å»é‡æ£€æµ‹å¤±è´¥: {e}ï¼Œé»˜è®¤ä¸é‡å¤")
        return False, f"æ£€æµ‹å¤±è´¥: {e}"


def _extract_time_info(memory_content: str, memory_key: str, mem_cube, mem_cube_id: str, llm) -> tuple[bool, str]:
    """
    ä»å†å²äº‹å®è®°å¿†ä¸­æå–å¹¶æ€»ç»“æ—¶é—´ä¿¡æ¯
    
    Args:
        memory_content: è®°å¿†å†…å®¹
        memory_key: è®°å¿†çš„keyï¼ˆå¦‚family_commute, delivery_patternç­‰ï¼‰
        mem_cube: MemCubeå®ä¾‹ï¼Œç”¨äºæ£€ç´¢å†å²è®°å¿†
        mem_cube_id: ç”¨æˆ·çš„mem_cube_id
        llm: LLMå®ä¾‹
        
    Returns:
        (needs_time, extracted_time): æ˜¯å¦éœ€è¦æ—¶é—´åŠæå–çš„æ—¶é—´ä¿¡æ¯
    """
    # åˆ¤æ–­æ˜¯å¦ä¸ºéœ€è¦æ—¶é—´çš„è®°å¿†ç±»å‹
    time_required_keys = [
        "family_commute", "door_usage", "door_state", "delivery_pattern"
    ]
    
    if memory_key not in time_required_keys:
        return False, ""
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»åŒ…å«æ—¶é—´èŒƒå›´ï¼ˆHH:MM-HH:MMæ ¼å¼ï¼‰
    time_pattern = r'\d{2}:\d{2}-\d{2}:\d{2}'
    if re.search(time_pattern, memory_content):
        return False, ""  # å·²ç»æœ‰æ—¶é—´ä¿¡æ¯
    
    # æ£€ç´¢ç›¸å…³çš„å†å²äº‹å®è®°å¿†
    try:
        # ä½¿ç”¨è®°å¿†å†…å®¹ä½œä¸ºæŸ¥è¯¢ï¼Œæ£€ç´¢ç›¸å…³çš„å†å²è®°å¿†
        related_memories = mem_cube.text_mem.search(
            query=memory_content,
            user_name=mem_cube_id,
            top_k=30,  # å¤šæ£€ç´¢ä¸€äº›äº‹å®è®°å¿†
        )
        
        # è¿‡æ»¤å‡ºäº‹å®è®°å¿†ï¼ˆFactual Memoryï¼‰å¹¶æå–æ—¶é—´æˆ³
        factual_memories_with_time = []
        for mem in related_memories:
            # æ£€æŸ¥æ˜¯å¦ä¸ºäº‹å®è®°å¿†
            is_factual = (
                "[å®æ—¶è®°å¿†]" in mem.memory or 
                "[Factual Memory]" in mem.memory
            )
            
            if is_factual:
                # å°è¯•ä» metadata ä¸­æå–æ—¶é—´æˆ³
                timestamp = None
                if hasattr(mem, 'metadata'):
                    # å°è¯•å¤šç§æ–¹å¼è·å–æ—¶é—´æˆ³
                    if hasattr(mem.metadata, 'sources') and mem.metadata.sources:
                        for source in mem.metadata.sources:
                            if isinstance(source, dict):
                                # å°è¯•ä» current_event æˆ–å…¶ä»–å­—æ®µæå–æ—¶é—´
                                current_event = source.get('current_event', '')
                                if current_event and '[' in current_event:
                                    # å‡è®¾æ ¼å¼ï¼š[HH:MM | ... ] æˆ– [YYYY-MM-DD HH:MM]
                                    time_match = re.search(r'\[(\d{2}:\d{2})', current_event)
                                    if time_match:
                                        timestamp = time_match.group(1)
                                        break
                                # æˆ–è€…ç›´æ¥ä» metadata å­—æ®µè·å–
                                if 'timestamp' in source:
                                    ts = source['timestamp']
                                    # æå–æ—¶é—´éƒ¨åˆ†ï¼ˆHH:MMï¼‰
                                    time_match = re.search(r'(\d{2}:\d{2})', str(ts))
                                    if time_match:
                                        timestamp = time_match.group(1)
                                        break
                
                if timestamp:
                    factual_memories_with_time.append({
                        "content": mem.memory,
                        "timestamp": timestamp
                    })
        
        if not factual_memories_with_time:
            logger.info(f"æœªæ‰¾åˆ°å¸¦æ—¶é—´æˆ³çš„ç›¸å…³äº‹å®è®°å¿†ï¼ŒKey: {memory_key}")
            return False, ""
        
        # ä½¿ç”¨ LLM ä»è¿™äº›å¸¦æ—¶é—´æˆ³çš„äº‹å®è®°å¿†ä¸­æ€»ç»“æ—¶é—´è§„å¾‹
        memories_text = "\n".join([
            f"[{m['timestamp']}] {m['content']}" 
            for m in factual_memories_with_time[:20]  # æä¾›æ›´å¤šå€™é€‰ï¼Œè®© LLM ç­›é€‰
        ])
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ—¶é—´è§„å¾‹æå–ä¸“å®¶ã€‚è¯·ä»å†å²äº‹å®è®°å¿†ä¸­**è°¨æ…åœ°**æå–æ—¶é—´è§„å¾‹ã€‚

ã€é‡è¦æé†’ã€‘
âš ï¸ æ£€ç´¢å‡ºæ¥çš„å†å²è®°å¿†å¯èƒ½ä¸å®Œå…¨ç›¸å…³ï¼Œä½ éœ€è¦ï¼š
1. **é¦–å…ˆåˆ¤æ–­æ¯æ¡å†å²è®°å¿†æ˜¯å¦ä¸è§„å¾‹è®°å¿†çœŸæ­£ç›¸å…³**
2. **åªä½¿ç”¨ç›¸å…³çš„äº‹ä»¶æ¥æå–æ—¶é—´**
3. **ç¡®ä¿æå–çš„æ—¶é—´å…·æœ‰é«˜ç½®ä¿¡åº¦**ï¼ˆè‡³å°‘3-5æ¡ç›¸å…³äº‹ä»¶æ”¯æŒï¼‰
4. **å¦‚æœç›¸å…³äº‹ä»¶å°‘äº3æ¡ï¼Œæˆ–è€…æ—¶é—´åˆ†æ•£ä¸æˆè§„å¾‹ï¼Œè¯·è¿”å› has_time: false**

ã€è§„å¾‹è®°å¿†ç±»å‹ã€‘
{memory_key}

ã€è§„å¾‹è®°å¿†å†…å®¹ã€‘
{memory_content}

ã€æ£€ç´¢åˆ°çš„å†å²äº‹å®è®°å¿†ï¼ˆå¸¦æ—¶é—´æˆ³ï¼Œå¯èƒ½ä¸å…¨éƒ¨ç›¸å…³ï¼‰ã€‘
{memories_text}

ã€æå–æ­¥éª¤ã€‘
æ­¥éª¤1: ç­›é€‰ç›¸å…³äº‹ä»¶
- ä»”ç»†é˜…è¯»è§„å¾‹è®°å¿†çš„æè¿°ï¼ˆå¦‚"å®¶åº­æˆå‘˜ç¦»å¼€"ã€"è¿”å›"ç­‰ï¼‰
- ä»å†å²è®°å¿†ä¸­æ‰¾å‡º**çœŸæ­£ç›¸å…³**çš„äº‹ä»¶ï¼ˆå†…å®¹è¯­ä¹‰åŒ¹é…ï¼‰
- å¿½ç•¥ä¸ç›¸å…³çš„äº‹ä»¶ï¼ˆä¾‹å¦‚ï¼šè§„å¾‹æ˜¯"ç¦»å¼€"ï¼Œä½†äº‹ä»¶æ˜¯"å–‚å® ç‰©"ï¼‰

æ­¥éª¤2: åˆ¤æ–­æ˜¯å¦è¶³å¤Ÿ
- ç›¸å…³äº‹ä»¶ â‰¥ 3æ¡ï¼šç»§ç»­
- ç›¸å…³äº‹ä»¶ < 3æ¡ï¼šè¿”å› has_time: false

æ­¥éª¤3: æå–æ—¶é—´è§„å¾‹
- åˆ†æç›¸å…³äº‹ä»¶çš„æ—¶é—´æˆ³
- å¦‚æœæ˜¯"ç¦»å¼€å’Œè¿”å›"ç±»å‹ï¼Œéœ€è¦åˆ†åˆ«å¤„ç†ç¦»å¼€æ—¶é—´å’Œè¿”å›æ—¶é—´
- æ‰¾å‡ºæ—¶é—´èŒƒå›´ï¼ˆæœ€æ—©åˆ°æœ€æ™šï¼Œå¯é€‚å½“æ‰©å±•ï¼‰
- ç¡®ä¿æ—¶é—´èŒƒå›´åˆç†ï¼ˆå¦‚ 06:00-10:00ï¼Œè€Œä¸æ˜¯ 06:45-06:50ï¼‰

æ­¥éª¤4: è¯„ä¼°ç½®ä¿¡åº¦
- high: â‰¥5æ¡ç›¸å…³äº‹ä»¶ï¼Œæ—¶é—´é›†ä¸­åœ¨2-4å°æ—¶çª—å£å†…
- medium: 3-4æ¡ç›¸å…³äº‹ä»¶ï¼Œæ—¶é—´ç›¸å¯¹é›†ä¸­
- low: æ—¶é—´å¤ªåˆ†æ•£æˆ–äº‹ä»¶å¤ªå°‘
- å¦‚æœæ˜¯ lowï¼Œè¯·è¿”å› has_time: false

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{{
  "has_time": true/false,
  "time_range": "HH:MM-HH:MM",
  "confidence": "high/medium",
  "relevant_count": <ç›¸å…³äº‹ä»¶æ•°é‡>,
  "explanation": "ä»Xæ¡æ£€ç´¢è®°å¿†ä¸­ç­›é€‰å‡ºYæ¡ç›¸å…³äº‹ä»¶ï¼Œæ—¶é—´èŒƒå›´ä¸º..."
}}

ã€ç¤ºä¾‹ã€‘
å‡è®¾è§„å¾‹è®°å¿†æ˜¯ï¼š"å®¶åº­æˆå‘˜ç»å¸¸åœ¨æ—©ä¸Šç¦»å¼€ä½æ‰€"
- ç›¸å…³äº‹ä»¶ï¼š[07:30] ç”·æ€§ç¦»å¼€, [08:15] å¥³æ€§ç¦»å¼€, [08:45] ç”·æ€§å¼€è½¦ç¦»å¼€
- ä¸ç›¸å…³äº‹ä»¶ï¼š[16:30] å–‚çŒ«, [20:00] è½¦åœåœ¨è½¦åº“
- æå–ç»“æœï¼štime_range: "07:00-09:00", confidence: "high", relevant_count: 3
"""
        
        # æ‰“å° prompt åˆ°æ—¥å¿—ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        print(f"\n{'='*80}")
        print(f"ã€æ—¶é—´æå– LLM Promptã€‘")
        print(f"{'='*80}")
        print(f"è®°å¿†ç±»å‹: {memory_key}")
        print(f"è§„å¾‹å†…å®¹: {memory_content[:150]}...")
        print(f"æ£€ç´¢åˆ° {len(factual_memories_with_time)} æ¡å¸¦æ—¶é—´æˆ³çš„äº‹å®è®°å¿†")
        print(f"\nPrompt (å‰1500å­—ç¬¦):")
        print(prompt[:1500])
        if len(prompt) > 1500:
            print(f"... (å‰©ä½™ {len(prompt) - 1500} ä¸ªå­—ç¬¦)")
        print(f"{'='*80}\n")
        
        logger.info(
            f"å¼€å§‹æ—¶é—´æå– - Key: {memory_key}, "
            f"æ£€ç´¢åˆ° {len(factual_memories_with_time)} æ¡å¸¦æ—¶é—´æˆ³çš„äº‹å®è®°å¿†"
        )
        
        messages = [{"role": "user", "content": prompt}]
        response = llm.generate(messages)
        
        # æ¸…ç†å“åº”
        response_clean = response.strip()
        if response_clean.startswith("```json"):
            response_clean = response_clean[7:]
        if response_clean.startswith("```"):
            response_clean = response_clean[3:]
        if response_clean.endswith("```"):
            response_clean = response_clean[:-3]
        response_clean = response_clean.strip()
        
        result = json.loads(response_clean)
        has_time = result.get("has_time", False)
        time_range = result.get("time_range", "")
        confidence = result.get("confidence", "low")
        relevant_count = result.get("relevant_count", 0)
        explanation = result.get("explanation", "")
        
        # æ‰“å° LLM å“åº”ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        print(f"\n{'='*80}")
        print(f"ã€æ—¶é—´æå– LLM Responseã€‘")
        print(f"{'='*80}")
        print(f"Has Time: {has_time}")
        print(f"Time Range: {time_range}")
        print(f"Confidence: {confidence}")
        print(f"Relevant Count: {relevant_count}")
        print(f"Explanation: {explanation}")
        print(f"{'='*80}\n")
        
        logger.info(
            f"æ—¶é—´æå–ç»“æœ - Key: {memory_key}, HasTime: {has_time}, "
            f"TimeRange: {time_range}, Confidence: {confidence}, "
            f"æ£€ç´¢ {len(factual_memories_with_time)} æ¡ -> ç›¸å…³ {relevant_count} æ¡, "
            f"Explanation: {explanation}"
        )
        
        # åªæœ‰åœ¨ç½®ä¿¡åº¦ä¸º high æˆ– medium æ—¶æ‰è¿”å›æ—¶é—´
        if has_time and confidence in ["high", "medium"] and relevant_count >= 3:
            return True, time_range
        else:
            logger.info(f"æ—¶é—´æå–ç½®ä¿¡åº¦ä¸è¶³æˆ–ç›¸å…³äº‹ä»¶å¤ªå°‘ï¼Œä¸æ·»åŠ æ—¶é—´ä¿¡æ¯")
            return False, ""
        
    except Exception as e:
        logger.warning(f"æ—¶é—´æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, ""


@router.post("/add", summary="Add memories", response_model=MemoryResponse)
def add_memories(add_req: APIADDRequest):
    """Add memories for a specific user."""
    # Create UserContext object - how to assign values
    user_context = UserContext(
        user_id=add_req.user_id,
        mem_cube_id=add_req.mem_cube_id,
        session_id=add_req.session_id or "default_session",
    )
    target_session_id = add_req.session_id
    if not target_session_id:
        target_session_id = "default_session"

    def _process_text_mem() -> list[dict[str, str]]:
        # Determine the type based on source or default to chat
        mem_type = "chat"
        if add_req.source and "security" in add_req.source.lower():
            mem_type = "security"
        elif add_req.source and "anker" in add_req.source.lower():
            mem_type = "security"
        
        # For security type, retrieve historical similar events
        info_dict = {
            "user_id": add_req.user_id,
            "session_id": target_session_id,
        }
        
        # ç”¨äºå­˜å‚¨æ£€ç´¢åˆ°çš„å†å²è®°å¿†ï¼ˆç”¨äºè¿”å›ç»™è°ƒç”¨è€…ï¼‰
        retrieved_historical_memories = []
        
        if mem_type == "security" and add_req.messages:
            # Get current event content
            current_event = add_req.messages[0].get("content", "")
            
            # Search for similar historical events
            try:
                similar_memories = naive_mem_cube.text_mem.search(
                    query=current_event,
                    user_name=user_context.mem_cube_id,
                    top_k=20,  # å¤šæ£€ç´¢ä¸€äº›ï¼Œåé¢ä¼šè¿‡æ»¤
                )
                # è¿‡æ»¤æ‰æ¨ç†æ€§å†…å®¹ï¼Œä¿ç•™äº‹å®å’Œè§„å¾‹è®°å¿†ï¼ˆåŒ…æ‹¬å¯èƒ½ä¸å¤ªç›¸å…³çš„ï¼‰
                filtered_memories = []
                
                for mem in similar_memories:
                    # è·å–ç›¸ä¼¼åº¦åˆ†æ•°
                    similarity = getattr(mem, 'similarity', None)
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºæ¨ç†æ€§å†…å®¹
                    is_inference = (
                        "[æ¨ç†è®°å¿†]" in mem.memory or 
                        "[Inference Memory]" in mem.memory or
                        "ã€æ¨æµ‹ã€‘" in mem.memory or 
                        "[æ¨æµ‹]" in mem.memory or 
                        "ã€æ¨ç†ã€‘" in mem.memory or
                        "inference" in mem.metadata.tags
                    )
                    
                    # åªä¿ç•™éæ¨ç†æ€§å†…å®¹ï¼ˆäº‹å®è®°å¿†å’Œè§„å¾‹è®°å¿†ï¼‰
                    # æ³¨æ„ï¼šä¸è¿‡æ»¤ç›¸ä¼¼åº¦ï¼Œè®© LLM è‡ªå·±åˆ¤æ–­æ˜¯å¦ç›¸å…³
                    if not is_inference:
                        filtered_memories.append(mem)
                        # ä¿å­˜ç”¨äºè¿”å›
                        retrieved_historical_memories.append({
                            "memory": mem.memory,
                            "memory_id": mem.id,
                            "similarity": similarity
                        })
                        if len(filtered_memories) >= 20:  # æœ€å¤šå–8ä¸ªï¼Œè®© LLM æœ‰æ›´å¤šé€‰æ‹©
                            break
                
                # Format historical events for prompt (with timestamps and hour)
                historical_events = ""
                for mem in filtered_memories:
                    # å°è¯•ä» metadata ä¸­è·å–æ—¶é—´æˆ³
                    timestamp_info = ""
                    if hasattr(mem, 'metadata') and hasattr(mem.metadata, 'sources'):
                        for source in mem.metadata.sources:
                            if isinstance(source, dict) and 'current_event' in source:
                                # ä» current_event ä¸­æå–æ—¶é—´æˆ³ï¼ˆå‡è®¾æ ¼å¼åŒ…å«æ—¶é—´ï¼‰
                                source_event = source['current_event']  # ä¿®å¤ï¼šä½¿ç”¨ä¸åŒçš„å˜é‡åï¼Œé¿å…è¦†ç›–å¤–å±‚çš„ current_event
                                # å°è¯•åŒ¹é…æ—¥æœŸæ—¶é—´æ ¼å¼ YYYY-MM-DD HH:MM
                                datetime_match = re.search(r'(\d{4}-\d{2}-\d{2})\s+(\d{2}):(\d{2})', source_event)
                                if datetime_match:
                                    date = datetime_match.group(1)
                                    hour = datetime_match.group(2)
                                    minute = datetime_match.group(3)
                                    # åŠ ä¸Šå½“å¤©å°æ—¶ä¿¡æ¯
                                    timestamp_info = f"[{date} {hour}:{minute} ({hour}h)] "
                                else:
                                    # å¦‚æœæ²¡æœ‰æ—¶é—´ï¼Œè‡³å°‘æå–æ—¥æœŸ
                                    date_match = re.search(r'(\d{4}-\d{2}-\d{2})', source_event)
                                    if date_match:
                                        timestamp_info = f"[{date_match.group(1)}] "
                                break
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¶é—´æˆ³ï¼Œå°è¯•ä» created_at è·å–
                    if not timestamp_info and hasattr(mem, 'metadata') and hasattr(mem.metadata, 'created_at'):
                        created_at = mem.metadata.created_at
                        if created_at:
                            # å–æ—¥æœŸå’Œå°æ—¶åˆ†é’Ÿéƒ¨åˆ†ï¼Œå¹¶åŠ ä¸Šå°æ—¶ä¿¡æ¯
                            created_str = str(created_at)[:16]
                            hour_match = re.search(r'(\d{2}):\d{2}$', created_str)
                            if hour_match:
                                hour = hour_match.group(1)
                                timestamp_info = f"[{created_str} ({hour}h)] "
                            else:
                                timestamp_info = f"[{created_str}] "
                    
                    historical_events += timestamp_info + mem.memory + "\n"
                
                if historical_events:
                    info_dict["historical_events"] = historical_events
                    logger.info(
                        f"Retrieved {len(filtered_memories)} non-inference historical events for pattern extraction "
                        f"(filtered from {len(similar_memories)} total)"
                    )
            except Exception as e:
                logger.warning(f"Failed to retrieve historical events: {e}")
        
        memories_local = mem_reader.get_memory(
            [add_req.messages],
            type=mem_type,
            info=info_dict,
        )
        flattened_local = [mm for m in memories_local for mm in m]
        logger.info(f"Memory extraction completed for user {add_req.user_id} using type={mem_type}")
        
        # ğŸ” å¯¹éFactual Memoryè¿›è¡Œå»é‡æ£€æµ‹å’Œæ—¶é—´æå–
        filtered_memories = []
        duplicate_memories = []  # è®°å½•é‡å¤çš„è®°å¿†
        total_memories = len(flattened_local)
        duplicate_count = 0
        
        for memory_item in flattened_local:
            memory_content = memory_item.memory
            
            # åˆ¤æ–­è®°å¿†ç±»å‹
            is_factual = "[å®æ—¶è®°å¿†]" in memory_content or "[Factual Memory]" in memory_content
            is_pattern = "[è§„å¾‹è®°å¿†]" in memory_content or "[Pattern Memory]" in memory_content
            is_inference = "[æ¨ç†è®°å¿†]" in memory_content or "[Inference Memory]" in memory_content
            
            # å¦‚æœæ˜¯Factual Memoryï¼Œç›´æ¥æ·»åŠ 
            if is_factual:
                logger.info(f"âœ… Factual Memoryï¼Œç›´æ¥æ·»åŠ : {memory_content[:100]}...")
                filtered_memories.append(memory_item)
                continue
            
            # å¯¹Pattern Memoryå’ŒInference Memoryè¿›è¡Œå¤„ç†
            if is_pattern or is_inference:
                memory_type_label = "Pattern Memory" if is_pattern else "Inference Memory"
                logger.info(f"ğŸ” æ£€æµ‹åˆ° {memory_type_label}ï¼Œå¼€å§‹å¤„ç†...")
                
                # 1. å»é‡æ£€æµ‹
                is_duplicate = False
                try:
                    logger.info(f"ğŸ” æ£€æµ‹ {memory_type_label} å»é‡...")
                    
                    # æ£€ç´¢è¯¥ç”¨æˆ·æ‰€æœ‰éFactualçš„è®°å¿†ï¼ˆå¢åŠ  top_k é¿å…é—æ¼é‡å¤ï¼‰
                    all_memories = naive_mem_cube.text_mem.search(
                        query=memory_content,
                        user_name=user_context.mem_cube_id,
                        top_k=50,  # å¢åŠ æ£€ç´¢æ•°é‡ï¼Œç¡®ä¿ä¸é—æ¼é‡å¤è®°å¿†
                    )
                    
                    # è¿‡æ»¤å‡ºéFactual Memory
                    non_factual_memories = [
                        mem for mem in all_memories
                        if not ("[å®æ—¶è®°å¿†]" in mem.memory or "[Factual Memory]" in mem.memory)
                    ]
                    
                    logger.info(f"   æ£€ç´¢åˆ° {len(non_factual_memories)} æ¡éFactualè®°å¿†")
                    
                    if non_factual_memories:
                        is_duplicate, dup_reason = _check_memory_duplication(
                            memory_content, non_factual_memories, llm
                        )
                        
                        if is_duplicate:
                            logger.info(f"   åˆ¤å®š: é‡å¤ - {dup_reason}")
                            duplicate_count += 1
                            duplicate_memories.append({
                                'content': memory_content,
                                'reason': dup_reason
                            })
                            continue  # è·³è¿‡è¿™æ¡é‡å¤è®°å¿†
                        else:
                            logger.info(f"   åˆ¤å®š: ä¸é‡å¤ - {dup_reason}")
                    else:
                        # æ²¡æœ‰å†å²è®°å¿†ï¼Œç›´æ¥æ·»åŠ 
                        logger.info(f"   åˆ¤å®š: ä¸é‡å¤ï¼ˆæ— å†å²è®°å¿†ï¼‰")
                except Exception as e:
                    error_msg = f"âš ï¸  å»é‡æ£€æµ‹å¼‚å¸¸: {e}ï¼Œé»˜è®¤æ·»åŠ è¯¥è®°å¿†"
                    logger.warning(error_msg)
                
                # 2. æ—¶é—´ä¿¡æ¯æå–
                try:
                    # è·å–è®°å¿†çš„keyï¼ˆä»metadataä¸­è·å–ï¼‰
                    memory_key = getattr(memory_item.metadata, 'key', '')
                    
                    if memory_key:
                        needs_time, time_range = _extract_time_info(
                            memory_content, memory_key, naive_mem_cube, user_context.mem_cube_id, llm
                        )
                        
                        if needs_time and time_range:
                            # æ›´æ–°è®°å¿†å†…å®¹ï¼Œæ·»åŠ æ—¶é—´ä¿¡æ¯
                            # åœ¨è®°å¿†ç±»å‹æ ‡ç­¾åæ·»åŠ æ—¶é—´ä¿¡æ¯
                            if "[Pattern Memory]" in memory_content:
                                updated_content = memory_content.replace(
                                    "[Pattern Memory]",
                                    f"[Pattern Memory] (Time: {time_range})"
                                )
                            elif "[è§„å¾‹è®°å¿†]" in memory_content:
                                updated_content = memory_content.replace(
                                    "[è§„å¾‹è®°å¿†]",
                                    f"[è§„å¾‹è®°å¿†] (æ—¶é—´: {time_range})"
                                )
                            elif "[Inference Memory]" in memory_content:
                                updated_content = memory_content.replace(
                                    "[Inference Memory]",
                                    f"[Inference Memory] (Time: {time_range})"
                                )
                            elif "[æ¨ç†è®°å¿†]" in memory_content:
                                updated_content = memory_content.replace(
                                    "[æ¨ç†è®°å¿†]",
                                    f"[æ¨ç†è®°å¿†] (æ—¶é—´: {time_range})"
                                )
                            else:
                                updated_content = memory_content
                            
                            memory_item.memory = updated_content
                            logger.info(f"   æ—¶é—´æå–: {memory_key} -> {time_range}")
                            print(f"   â° æ·»åŠ æ—¶é—´: {time_range}")
                except Exception as e:
                    time_error_msg = f"âš ï¸  æ—¶é—´æå–å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å†…å®¹"
                    logger.warning(time_error_msg)
                    print(f"   {time_error_msg}")
                
                # æ·»åŠ åˆ°è¿‡æ»¤åçš„åˆ—è¡¨
                filtered_memories.append(memory_item)
            else:
                # å…¶ä»–ç±»å‹çš„è®°å¿†ç›´æ¥æ·»åŠ 
                filtered_memories.append(memory_item)
        
        # ä½¿ç”¨è¿‡æ»¤å’Œæ›´æ–°åçš„è®°å¿†åˆ—è¡¨
        flattened_local = filtered_memories
        added_count = len(filtered_memories)
        
        # æ‰“å°å»é‡ç»Ÿè®¡
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ã€å»é‡ç»Ÿè®¡ã€‘")
        print(f"{'='*60}")
        print(f"  æœ¬æ¬¡æå–è®°å¿†æ€»æ•°: {total_memories}")
        print(f"  åˆ¤å®šä¸ºé‡å¤: {duplicate_count} æ¡")
        print(f"  æœ€ç»ˆåŠ å…¥: {added_count} æ¡")
        print(f"{'='*60}\n")
        
        logger.info(
            f"å»é‡ç»Ÿè®¡ - æ€»æ•°: {total_memories}, é‡å¤: {duplicate_count}, åŠ å…¥: {added_count}"
        )
        
        mem_ids_local: list[str] = naive_mem_cube.text_mem.add(
            flattened_local,
            user_name=user_context.mem_cube_id,
        )
        logger.info(
            f"Added {len(mem_ids_local)} memories for user {add_req.user_id} "
            f"in session {add_req.session_id}: {mem_ids_local}"
        )
        
        # æ‰“å°æ¯æ¡æ·»åŠ åˆ°æ•°æ®åº“çš„è®°å¿†
        print(f"\n{'='*60}")
        print(f"âœ… ã€æˆåŠŸæ·»åŠ  {len(mem_ids_local)} æ¡è®°å¿†åˆ°æ•°æ®åº“ã€‘")
        print(f"{'='*60}")
        
        for idx, (memory_id, memory) in enumerate(zip(mem_ids_local, flattened_local), 1):
            memory_content = memory.memory
            
            # åˆ¤æ–­è®°å¿†ç±»å‹ï¼ˆç”¨äºå›¾æ ‡ï¼‰
            if "[å®æ—¶è®°å¿†]" in memory_content or "[Factual Memory]" in memory_content:
                mem_icon = "ğŸ“Œ"
                mem_label = "å®æ—¶è®°å¿†"
            elif "[è§„å¾‹è®°å¿†]" in memory_content or "[Pattern Memory]" in memory_content:
                mem_icon = "ğŸ”„"
                mem_label = "è§„å¾‹è®°å¿†"
            elif "[æ¨ç†è®°å¿†]" in memory_content or "[Inference Memory]" in memory_content:
                mem_icon = "ğŸ¤”"
                mem_label = "æ¨ç†è®°å¿†"
            else:
                mem_icon = "ğŸ“"
                mem_label = memory.metadata.memory_type
            
            print(f"\n{mem_icon} è®°å¿† {idx}: {mem_label}")
            print(f"  ID: {memory_id}")
            print(f"  å†…å®¹: {memory_content}")
        
        print(f"\n{'='*60}")
        
        # æ‰“å°é‡å¤çš„è®°å¿†ï¼ˆå¦‚æœæœ‰ï¼‰
        if duplicate_memories:
            print(f"\n{'='*60}")
            print(f"âŒ ã€é‡å¤çš„è®°å¿†ï¼ˆæœªåŠ å…¥ï¼‰ã€‘å…± {len(duplicate_memories)} æ¡")
            print(f"{'='*60}")
            for idx, dup_mem in enumerate(duplicate_memories, 1):
                # åˆ¤æ–­è®°å¿†ç±»å‹
                dup_content = dup_mem['content']
                if "[è§„å¾‹è®°å¿†]" in dup_content or "[Pattern Memory]" in dup_content:
                    mem_icon = "ğŸ”„"
                    mem_label = "è§„å¾‹è®°å¿†"
                elif "[æ¨ç†è®°å¿†]" in dup_content or "[Inference Memory]" in dup_content:
                    mem_icon = "ğŸ¤”"
                    mem_label = "æ¨ç†è®°å¿†"
                else:
                    mem_icon = "ğŸ“"
                    mem_label = "å…¶ä»–è®°å¿†"
                
                print(f"\nâŒ {mem_icon} [é‡å¤ {idx}]: {mem_label}")
                print(f"  å†…å®¹: {dup_content}")
                print(f"  åŸå› : {dup_mem['reason']}")
            print(f"\n{'='*60}")
        
        print()
        
        # æ„å»ºè¿”å›ç»“æœï¼ŒåŒ…å«æ£€ç´¢åˆ°çš„å†å²è®°å¿†
        result_memories = []
        for memory_id, memory in zip(mem_ids_local, flattened_local, strict=False):
            mem_dict = {
                "memory": memory.memory,
                "memory_id": memory_id,
                "memory_type": memory.metadata.memory_type,
            }
            # å¦‚æœæœ‰æ£€ç´¢åˆ°çš„å†å²è®°å¿†ï¼Œæ·»åŠ åˆ°ç¬¬ä¸€ä¸ªè®°å¿†é¡¹ä¸­
            if retrieved_historical_memories and len(result_memories) == 0:
                mem_dict["retrieved_historical_memories"] = retrieved_historical_memories
            result_memories.append(mem_dict)
        
        return result_memories

    def _process_pref_mem() -> list[dict[str, str]]:
        if os.getenv("ENABLE_PREFERENCE_MEMORY", "false").lower() != "true":
            return []
        pref_memories_local = naive_mem_cube.pref_mem.get_memory(
            [add_req.messages],
            type="chat",
            info={
                "user_id": add_req.user_id,
                "session_id": target_session_id,
            },
        )
        pref_ids_local: list[str] = naive_mem_cube.pref_mem.add(pref_memories_local)
        logger.info(
            f"Added {len(pref_ids_local)} preferences for user {add_req.user_id} "
            f"in session {add_req.session_id}: {pref_ids_local}"
        )
        return [
            {
                "memory": memory.memory,
                "memory_id": memory_id,
                "memory_type": memory.metadata.preference_type,
            }
            for memory_id, memory in zip(pref_ids_local, pref_memories_local, strict=False)
        ]

    with ContextThreadPoolExecutor(max_workers=2) as executor:
        text_future = executor.submit(_process_text_mem)
        pref_future = executor.submit(_process_pref_mem)
        text_response_data = text_future.result()
        pref_response_data = pref_future.result()

    return MemoryResponse(
        message="Memory added successfully",
        data=text_response_data + pref_response_data,
    )


@router.post("/chat/complete", summary="Chat with MemOS (Complete Response)")
def chat_complete(chat_req: APIChatCompleteRequest):
    """Chat with MemOS for a specific user. Returns complete response (non-streaming)."""
    try:
        # Collect all responses from the generator
        content, references = mos_server.chat(
            query=chat_req.query,
            user_id=chat_req.user_id,
            cube_id=chat_req.mem_cube_id,
            mem_cube=naive_mem_cube,
            history=chat_req.history,
            internet_search=chat_req.internet_search,
            moscube=chat_req.moscube,
            base_prompt=chat_req.base_prompt,
            top_k=chat_req.top_k,
            threshold=chat_req.threshold,
            session_id=chat_req.session_id,
        )

        # Return the complete response
        return {
            "message": "Chat completed successfully",
            "data": {"response": content, "references": references},
        }

    except ValueError as err:
        raise HTTPException(status_code=404, detail=str(traceback.format_exc())) from err
    except Exception as err:
        logger.error(f"Failed to start chat: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(traceback.format_exc())) from err
