"""
Anker å®¶åº­å®‰é˜² - æ¸è¿›å¼èº«ä»½æ¨ç†ä¸è¯„ä¼°ç³»ç»Ÿ

æ ¸å¿ƒæµç¨‹:
1. æ•°æ®æ’åº: ä»æ•°æ®é‡å°‘çš„å®¶åº­å¼€å§‹å¤„ç†
2. æ¸è¿›å¼å­¦ä¹ :
   - å°†9æœˆæ•°æ®åˆ’åˆ†ä¸º: è®­ç»ƒæµ + éªŒè¯é›†(éšæœºé‡‡æ ·50æ¡ï¼Œæ— äº¤é›†)
   - é€æ­¥å°†è®­ç»ƒæµåŠ å…¥è®°å¿†ç³»ç»Ÿ(åˆ†å¤šä¸ªé˜¶æ®µ)
   - æ¯æ¬¡åŠ å…¥å,åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°å‡†ç¡®ç‡æŒ‡æ ‡
3. æœ€ç»ˆæ¨ç†:
   - è¡¥å……å®Œæ•´9æœˆè®°å¿†(åŒ…æ‹¬éªŒè¯é›†)
   - å¯¹10æœˆæ•°æ®è¿›è¡Œå…¨é‡æ¨ç†

è¯„ä¼°æŒ‡æ ‡:
- èº«ä»½å¤§ç±»å‡†ç¡®ç‡ (Role Type Accuracy)
- èº«ä»½å­ç±»å‡†ç¡®ç‡ (Sub-role Type Accuracy)
- å®¶äººè¯†åˆ«å‡†ç¡®ç‡ (Family Member Recognition)
- å¼‚å¸¸æ£€æµ‹å‡†ç¡®ç‡ (Anomaly Detection)
"""

import json
import sys
import time
import re
import math
import random
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any
from collections import defaultdict
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé¿å…åœ¨éä¸»çº¿ç¨‹åˆ›å»ºGUI
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from memos.api.product_models import APIADDRequest, APIChatCompleteRequest
from memos.api.routers.server_router import add_memories, chat_complete, naive_mem_cube, llm
from memos.log import get_logger

logger = get_logger(__name__)

# æ•°æ®é…ç½®
DATA_DIR = project_root / "evaluation" / "data" / "anker" / "Test_data_22_pu_3_family_mothes_seperated"

# å®¶åº­IDåˆ—è¡¨ (æŒ‰æ•°æ®é‡ä»å°åˆ°å¤§æ’åº: F3 < F1 < F2)
FAMILY_ORDER = [
    "T8030P232228002B",  # ~1.8k events
    "T8030P1322100087",  # ~4.1k events
    "T8030P132215001F"   # ~5.7k events
]

class MetricsCalculator:
    """æŒ‡æ ‡è®¡ç®—å™¨"""
    
    @staticmethod
    def calculate(predictions: List[Dict], ground_truths: Dict[str, Dict]) -> Dict[str, float]:
        if not predictions:
            return {}
            
        total = 0
        correct_role = 0
        correct_sub_role = 0
        
        # å®¶äººè¯†åˆ« (Binary: Family vs Non-Family)
        family_tp = 0
        family_tn = 0
        family_fp = 0
        family_fn = 0
        
        # å¼‚å¸¸æ£€æµ‹ (Binary: Abnormal vs Normal)
        # Abnormal = Suspicious Person OR Critical Non-Human Event
        anomaly_tp = 0
        anomaly_tn = 0
        anomaly_fp = 0
        anomaly_fn = 0
        
        for pred in predictions:
            vid = pred['video_path']
            if vid not in ground_truths:
                continue
                
            total += 1
            gt = ground_truths[vid]
            
            # 1. åŸºç¡€å‡†ç¡®ç‡
            p_role = pred['predicted_role_type'].strip().lower()
            p_sub = pred['predicted_sub_role_type'].strip().lower()
            g_role = gt['role_type'].strip().lower()
            g_sub = gt['sub_role_type'].strip().lower()
            
            if p_role == g_role:
                correct_role += 1
            if p_sub == g_sub:
                correct_sub_role += 1
                
            # 2. å®¶äººè¯†åˆ«
            is_family_pred = (p_sub == 'family member')
            is_family_gt = (g_sub == 'family member')
            
            if is_family_pred and is_family_gt: family_tp += 1
            elif not is_family_pred and not is_family_gt: family_tn += 1
            elif is_family_pred and not is_family_gt: family_fp += 1
            elif not is_family_pred and is_family_gt: family_fn += 1
            
            # 3. å¼‚å¸¸æ£€æµ‹
            # å®šä¹‰å¼‚å¸¸: role_type=Suspicious Person æˆ– sub_role_type=Critical Non-Human Event
            is_anomaly_gt = (g_role == 'suspicious person' or g_sub == 'critical non-human event')
            is_anomaly_pred = (p_role == 'suspicious person' or p_sub == 'critical non-human event')
            
            if is_anomaly_pred and is_anomaly_gt: anomaly_tp += 1
            elif not is_anomaly_pred and not is_anomaly_gt: anomaly_tn += 1
            elif is_anomaly_pred and not is_anomaly_gt: anomaly_fp += 1
            elif not is_anomaly_pred and is_anomaly_gt: anomaly_fn += 1

        # è®¡ç®—æŒ‡æ ‡
        def safe_div(a, b): return a / b if b > 0 else 0
        
        return {
            "role_acc": safe_div(correct_role, total),
            "sub_role_acc": safe_div(correct_sub_role, total),
            "family_acc": safe_div(family_tp + family_tn, total),
            "family_f1": safe_div(2 * family_tp, 2 * family_tp + family_fp + family_fn),
            "anomaly_acc": safe_div(anomaly_tp + anomaly_tn, total),
            "anomaly_recall": safe_div(anomaly_tp, anomaly_tp + anomaly_fn)
        }

def load_data(family_id: str) -> Tuple[List[Dict], List[Dict]]:
    """åŠ è½½æ•°æ®: è¿”å› (9æœˆæ•°æ®, 10æœˆæ•°æ®)"""
    
    # 1. Load Train (Sep)
    train_path = DATA_DIR / family_id / f"{family_id}_09.json"
    with open(train_path, 'r', encoding='utf-8') as f:
        train_raw = json.load(f)
        
    train_events = []
    for path, data in train_raw.items():
        data['video_path'] = path
        train_events.append(data)
    # æŒ‰æ—¶é—´æ’åº
    train_events.sort(key=lambda x: x['timestamp'])
    
    # 2. Load Test (Oct)
    test_path = DATA_DIR / family_id / f"{family_id}_10_no_role.json"
    test_events = []
    if test_path.exists():
        with open(test_path, 'r', encoding='utf-8') as f:
            test_raw = json.load(f)
        for path, data in test_raw.items():
            data['video_path'] = path
            test_events.append(data)
        test_events.sort(key=lambda x: x['timestamp'])
        
    return train_events, test_events

def format_timestamp(ts):
    try:
        return datetime.strptime(ts, "%Y%m%d%H%M%S").strftime("%Y-%m-%d %H:%M:%S")
    except:
        return ts

def get_time_period(ts):
    try:
        h = int(ts[8:10])
        if 6 <= h < 12: return "Morning"
        elif 12 <= h < 18: return "Afternoon"
        elif 18 <= h < 22: return "Evening"
        return "Night"
    except:
        return "Unknown"


def _get_metadata_value(metadata, key):
    if metadata is None:
        return None
    if isinstance(metadata, dict):
        return metadata.get(key)
    return getattr(metadata, key, None)


def _extract_pattern_dimension(memory_item):
    metadata = getattr(memory_item, "metadata", None)
    key = _get_metadata_value(metadata, "key")
    if key:
        return key
    topic = _get_metadata_value(metadata, "topic")
    if topic:
        return topic
    memory_text = getattr(memory_item, "memory", "").lower()
    if "leave" in memory_text and "return" in memory_text:
        return "family_leave_return"
    if "delivery" in memory_text:
        return "delivery_pattern"
    if "visitor" in memory_text:
        return "visitor_pattern"
    return None


def merge_duplicate_patterns(user_id, mem_cube_id, session_id, min_cluster_size=3):
    """åˆå¹¶é‡å¤ç»´åº¦çš„è§„å¾‹è®°å¿†ï¼Œå¹¶åˆ é™¤æ—§çš„"""
    try:
        pattern_memories = naive_mem_cube.text_mem.search(
            query="[Pattern Memory]",
            user_name=mem_cube_id,
            top_k=200,
        )
    except Exception as e:
        logger.warning(f"æ— æ³•æ£€ç´¢ Pattern Memory è¿›è¡Œåˆå¹¶: {e}")
        return []
    
    clusters = defaultdict(list)
    for mem in pattern_memories:
        # è·³è¿‡å·²ç»åˆå¹¶è¿‡çš„è®°å¿†
        memory_text = getattr(mem, "memory", "")
        if "[Merged]" in memory_text or "MASTER PATTERN" in memory_text:
            continue
        key = _extract_pattern_dimension(mem)
        if not key:
            continue
        clusters[key].append(mem)
    
    merge_reports = []
    for key, mems in clusters.items():
        if len(mems) < min_cluster_size:
            continue
        report = _merge_pattern_cluster(key, mems, user_id, mem_cube_id, session_id)
        if report:
            merge_reports.append(report)
    
    if merge_reports:
        print("\n" + "â•" * 100)
        print("ğŸ§¹ ã€Pattern Memory Consolidation Summaryã€‘")
        for report in merge_reports:
            print(f"  Key: {report['key']}")
            print(f"    â• æ–°å¢è®°å¿†: {report['new_id']}")
            print(f"    â– åˆ é™¤æ—§è®°å¿†({len(report['deleted_ids'])}): {', '.join(report['deleted_ids'])}")
        print("â•" * 100 + "\n")
    
    return merge_reports


def _merge_pattern_cluster(key, mems, user_id, mem_cube_id, session_id):
    mems = sorted(
        mems,
        key=lambda m: _get_metadata_value(getattr(m, "metadata", None), "created_at") or "",
        reverse=True,
    )
    snippet_lines = []
    for mem in mems[:20]:
        created = _get_metadata_value(getattr(mem, "metadata", None), "created_at")
        snippet_lines.append(f"- ID:{getattr(mem, 'id', 'N/A')} | Time:{created} | {getattr(mem, 'memory', '')}")
    snippet_text = "\n".join(snippet_lines)
    
    prompt = f"""You are merging overlapping security pattern memories for a smart home.
Pattern dimension (key): {key}

Existing pattern memories:
{snippet_text}

Please consolidate them into a single, more accurate pattern.
Return JSON:
{{
  "merged_pattern": "<concise pattern sentence>",
  "time_range": "<HH:MM-HH:MM or description>",
  "evidence_summary": "<how many observations support it>",
  "confidence": "<High/Medium/Low>"
}}"""
    
    req = APIChatCompleteRequest(
        user_id=user_id,
        mem_cube_id=mem_cube_id,
        query=prompt,
        moscube=True,
        top_k=5,
        threshold=0.3,
        internet_search=False,
        session_id=session_id,
    )
    
    try:
        res = chat_complete(req)
        raw = res.get("data", {}).get("response", "") if isinstance(res, dict) else ""
        cleaned = raw.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        merged_result = json.loads(cleaned)
    except Exception as e:
        logger.warning(f"åˆå¹¶æ¨¡å¼ LLM å“åº”è§£æå¤±è´¥ï¼Œè·³è¿‡æ­¤ç»´åº¦ {key}: {e}")
        return None
    
    merged_pattern = merged_result.get("merged_pattern")
    if not merged_pattern:
        return None
    time_range = merged_result.get("time_range", "N/A")
    evidence_summary = merged_result.get("evidence_summary", "")
    confidence = merged_result.get("confidence", "Medium")
    
    merged_content = (
        f"[Pattern Memory][Merged][Key: {key}] {merged_pattern}\n"
        f"Time Range: {time_range}\n"
        f"Evidence: {evidence_summary}\n"
        f"Confidence: {confidence}\n"
        f"Source: Consolidated from {len(mems)} historical memories."
    )
    
    add_req = APIADDRequest(
        user_id=user_id,
        mem_cube_id=mem_cube_id,
        messages=[{"role": "user", "content": merged_content}],
        session_id=session_id,
        source="anker_security_merge",
    )
    add_res = add_memories(add_req)
    new_ids = []
    if add_res and add_res.data:
        new_ids = [entry.get("memory_id") for entry in add_res.data if entry.get("memory_id")]
    
    delete_ids = [getattr(mem, "id", None) for mem in mems if getattr(mem, "id", None)]
    if delete_ids:
        try:
            naive_mem_cube.text_mem.delete(delete_ids)
        except Exception as e:
            logger.warning(f"åˆ é™¤æ—§è®°å¿†å¤±è´¥ {delete_ids}: {e}")
    
    print("\n" + "-" * 100)
    print(f"ğŸ§¹ Pattern Consolidation - Key: {key}")
    print("   åˆ é™¤çš„è®°å¿†:")
    for mem in mems:
        print(f"     â€¢ {getattr(mem, 'id', 'N/A')}: {getattr(mem, 'memory', '')}")
    print("   æ–°å¢è®°å¿†:")
    for new_id in new_ids:
        print(f"     â• {new_id}: {merged_pattern}")
    print("-" * 100 + "\n")
    
    return {
        "key": key,
        "new_id": new_ids[0] if new_ids else "N/A",
        "deleted_ids": delete_ids,
        "merged_pattern": merged_pattern,
    }


def _search_memories_for_user(query, mem_cube_id, top_k=2000):
    """åŸºäº query å’Œ userï¼Œæ£€ç´¢è®°å¿†å¹¶å»é‡"""
    try:
        results = naive_mem_cube.text_mem.search(
            query=query,
            user_name=mem_cube_id,
            top_k=top_k,
        )
    except Exception as e:
        logger.warning(f"    âš ï¸ æ£€ç´¢è®°å¿†å¤±è´¥ ({query}): {e}")
        return []
    
    memories = []
    seen_ids = set()
    for mem in results or []:
        mem_id = getattr(mem, "id", None)
        if not mem_id or mem_id in seen_ids:
            continue
        seen_ids.add(mem_id)
        memories.append({
            "id": mem_id,
            "memory": getattr(mem, "memory", ""),
            "created_at": _get_metadata_value(getattr(mem, "metadata", None), "created_at") or "N/A"
        })
    return memories


def _filter_memories_by_tags(memories, tags):
    return [
        mem for mem in memories
        if any(tag in mem["memory"] for tag in tags)
    ]


def save_all_memories_and_stats(family_id, mem_cube_id, output_dir, family_logger=None, memory_stats=None):
    """ä¿å­˜å½“å‰å®¶åº­çš„è®°å¿†å¿«ç…§ï¼ˆäº‹å®/è§„å¾‹/æ¨ç†ï¼‰åŠç»Ÿè®¡ä¿¡æ¯"""
    log = family_logger if family_logger else logger
    
    try:
        factual_mems = _search_memories_for_user("[Factual Memory]", mem_cube_id)
        if not factual_mems:
            factual_mems = _search_memories_for_user("Ground Truth Label", mem_cube_id)
        factual_mems = _filter_memories_by_tags(factual_mems, ["[Factual Memory]", "Ground Truth Label"])
        
        pattern_mems = _search_memories_for_user("[Pattern Memory]", mem_cube_id)
        pattern_mems = _filter_memories_by_tags(pattern_mems, ["[Pattern Memory]", "[è§„å¾‹è®°å¿†]"])
        
        inference_mems = _search_memories_for_user("[Inference Memory]", mem_cube_id)
        inference_mems = _filter_memories_by_tags(inference_mems, ["[Inference Memory]", "[æ¨ç†è®°å¿†]"])
        
        memory_dir = output_dir / "memories"
        memory_dir.mkdir(parents=True, exist_ok=True)
        
        snapshot = {
            "family_id": family_id,
            "timestamp": datetime.now().isoformat(),
            "memory_counts": {
                "factual": len(factual_mems),
                "pattern": len(pattern_mems),
                "inference": len(inference_mems),
                "total_active": len(factual_mems) + len(pattern_mems) + len(inference_mems),
            },
            "deletion_stats": memory_stats or {"deletion_ops": 0, "deleted_count": 0},
            "memories": {
                "factual": factual_mems,
                "pattern": pattern_mems,
                "inference": inference_mems,
            },
        }
        
        with open(memory_dir / f"{family_id}_memories.json", 'w', encoding='utf-8') as f:
            json.dump(snapshot, f, indent=2, ensure_ascii=False)
        
        log.info(
            f"    ğŸ’¾ è®°å¿†å¿«ç…§å·²ä¿å­˜: memories/{family_id}_memories.json "
            f"(Factual={len(factual_mems)}, Pattern={len(pattern_mems)}, Infer={len(inference_mems)})"
        )
        if memory_stats:
            log.info(
                f"    ğŸ§¹ åˆ é™¤ç»Ÿè®¡: æ“ä½œ {memory_stats.get('deletion_ops', 0)} æ¬¡, "
                f"å…±åˆ é™¤ {memory_stats.get('deleted_count', 0)} æ¡è®°å¿†"
            )
        
    except Exception as e:
        log.error(f"    âŒ ä¿å­˜è®°å¿†å¿«ç…§å¤±è´¥: {e}")
        import traceback
        log.error(traceback.format_exc())


def plot_progress_metrics(progress_log, family_id, output_dir, mode_label, family_logger=None):
    """ç»˜åˆ¶å¹¶ä¿å­˜å‡†ç¡®ç‡è¿›åº¦æ›²çº¿å›¾
    
    Args:
        progress_log: åŒ…å«å„é˜¶æ®µæŒ‡æ ‡çš„åˆ—è¡¨
        family_id: å®¶åº­ID
        output_dir: è¾“å‡ºç›®å½•
        mode_label: æ¨¡å¼æ ‡ç­¾
        family_logger: å¯é€‰çš„loggerï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨å…¨å±€logger
    """
    if not progress_log:
        return None
    
    log = family_logger if family_logger else logger
    
    try:
        phases = [entry["phase"] for entry in progress_log]
        role_acc = [entry["metrics"]["role_acc"] * 100 for entry in progress_log]
        sub_acc = [entry["metrics"]["sub_role_acc"] * 100 for entry in progress_log]
        family_acc = [entry["metrics"]["family_acc"] * 100 for entry in progress_log]
        anomaly_acc = [entry["metrics"]["anomaly_acc"] * 100 for entry in progress_log]
        
        plt.figure(figsize=(10, 6))
        plt.plot(phases, role_acc, marker='o', label="Role Type")
        plt.plot(phases, sub_acc, marker='s', label="Sub-role")
        plt.plot(phases, family_acc, marker='^', label="Family Recognition")
        plt.plot(phases, anomaly_acc, marker='d', label="Anomaly Detection")
        plt.xlabel("Training Phase")
        plt.ylabel("Accuracy (%)")
        plt.title(f"{family_id} - Accuracy Progress ({mode_label})")
        plt.ylim(0, 100)
        plt.grid(True, linestyle="--", alpha=0.5)
        plt.legend()
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir.mkdir(parents=True, exist_ok=True)
        
        plot_path = output_dir / f"{family_id}_{mode_label.replace(' ', '_')}_accuracy_progress.png"
        plt.tight_layout()
        plt.savefig(plot_path, dpi=150)
        plt.close()
        log.info(f"ğŸ“ˆ è¿›åº¦æ›²çº¿å·²ä¿å­˜: {plot_path}")
        return plot_path
    except Exception as e:
        log.error(f"âŒ ç»˜åˆ¶è¿›åº¦æ›²çº¿å¤±è´¥: {e}")
        import traceback
        log.error(traceback.format_exc())
        return None

def clean_duplicate_dimension_memories(mem_cube_id, family_logger=None, memory_stats=None):
    """æ¸…ç†ç›¸åŒç»´åº¦çš„é‡å¤è®°å¿†ï¼Œåªä¿ç•™æœ€æ–°çš„
    
    è¿™ä¸ªå‡½æ•°ä¼šæ£€ç´¢æ‰€æœ‰ Pattern Memoryï¼ŒæŒ‰ç»´åº¦åˆ†ç»„ï¼Œ
    å¯¹äºæ¯ä¸ªç»´åº¦ï¼Œåªä¿ç•™åˆ›å»ºæ—¶é—´æœ€æ–°çš„è®°å¿†ï¼Œåˆ é™¤æ—§çš„ã€‚
    
    Args:
        mem_cube_id: è®°å¿†ç«‹æ–¹ä½“ID
        family_logger: æ—¥å¿—è®°å½•å™¨
        memory_stats: ç»Ÿè®¡å­—å…¸
    """
    log = family_logger if family_logger else logger
    
    try:
        # æ£€ç´¢æ‰€æœ‰ Pattern Memory
        pattern_memories = naive_mem_cube.text_mem.search(
            query="[Pattern Memory]",
            user_name=mem_cube_id,
            top_k=200,
        )
        
        if not pattern_memories:
            return
        
        # æŒ‰ç»´åº¦åˆ†ç»„
        dimension_groups = defaultdict(list)
        for mem in pattern_memories:
            memory_text = getattr(mem, "memory", "")
            # è·³è¿‡å·²ç»åˆå¹¶è¿‡çš„è®°å¿†
            if "[Merged]" in memory_text:
                continue
            
            # æå–ç»´åº¦
            dimension = _extract_pattern_dimension(mem)
            if not dimension:
                dimension = "unknown"
            
            dimension_groups[dimension].append(mem)
        
        # å¯¹äºæ¯ä¸ªç»´åº¦ï¼Œåªä¿ç•™æœ€æ–°çš„ä¸€æ¡
        total_deleted = 0
        for dimension, mems in dimension_groups.items():
            if len(mems) <= 1:
                continue  # åªæœ‰ä¸€æ¡ï¼Œä¸éœ€è¦æ¸…ç†
            
            # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„
            mems_sorted = sorted(
                mems,
                key=lambda m: _get_metadata_value(getattr(m, "metadata", None), "created_at") or "",
                reverse=True
            )
            
            # ä¿ç•™ç¬¬ä¸€æ¡ï¼ˆæœ€æ–°ï¼‰ï¼Œåˆ é™¤å…¶ä»–
            latest_mem = mems_sorted[0]
            old_mems = mems_sorted[1:]
            
            # åˆ é™¤æ—§è®°å¿†
            old_ids = [getattr(m, "id", None) for m in old_mems if getattr(m, "id", None)]
            if old_ids:
                try:
                    naive_mem_cube.text_mem.delete(old_ids)
                    deleted_count = len(old_ids)
                    total_deleted += deleted_count
                    
                    if memory_stats is not None:
                        memory_stats["deletion_ops"] += 1
                        memory_stats["deleted_count"] += deleted_count
                    
                    log.info(
                        f"    ğŸ§¹ ç»´åº¦ '{dimension}': ä¿ç•™æœ€æ–°è®°å¿† "
                        f"(ID: {getattr(latest_mem, 'id', 'N/A')[:8]}...), "
                        f"åˆ é™¤ {deleted_count} æ¡æ—§è®°å¿†"
                    )
                except Exception as e:
                    log.warning(f"    âš ï¸  åˆ é™¤ç»´åº¦ '{dimension}' çš„æ—§è®°å¿†å¤±è´¥: {e}")
        
        if total_deleted > 0:
            log.info(f"    âœ… æ€»å…±æ¸…ç†äº† {total_deleted} æ¡é‡å¤ç»´åº¦çš„æ—§è®°å¿†")
    
    except Exception as e:
        log.warning(f"    âš ï¸  æ¸…ç†é‡å¤ç»´åº¦è®°å¿†æ—¶å‡ºé”™: {e}")


def add_events_to_memory(
    events,
    user_id,
    mem_cube_id,
    session_id,
    phase_info=None,
    include_labels=False,
    clean_duplicates=True,
    family_logger=None,
    memory_stats=None,
):
    """æ‰¹é‡æ·»åŠ äº‹ä»¶åˆ°è®°å¿† (åŒ…å«å†…å®¹å¢å¼º + è¿›åº¦å±•ç¤º + è‡ªåŠ¨å»é‡)
    
    Args:
        phase_info: dict with keys: current_phase, total_phases, phase_start_time
        include_labels: æ˜¯å¦åŒ…å«æ ‡ç­¾ï¼ˆrole_type å’Œ sub_role_typeï¼‰ç”¨äºè®­ç»ƒå­¦ä¹ 
        clean_duplicates: æ˜¯å¦åœ¨æ·»åŠ åæ¸…ç†ç›¸åŒç»´åº¦çš„é‡å¤è®°å¿†ï¼ˆé»˜è®¤Trueï¼‰
        family_logger: æ—¥å¿—è®°å½•å™¨
        memory_stats: ç»Ÿè®¡å­—å…¸ï¼Œç”¨äºè®°å½•åˆ é™¤æ“ä½œæ¬¡æ•°å’Œæ•°é‡ {"deletion_ops": 0, "deleted_count": 0}
    """
    log = family_logger if family_logger else logger
    total = len(events)
    start_t = time.time()
    accumulated = 0.0
    
    for i, event in enumerate(events, 1):
        event_start = time.time()
        ts = event.get('timestamp', '')
        time_str = format_timestamp(ts)
        period = get_time_period(ts)
        
        # åŸºç¡€è§‚å¯Ÿå†…å®¹
        enhanced_content = f"""[Security Log]
Time: {time_str} ({period})
Observation: {event['original_description']}"""

        # å¦‚æœæ˜¯è®­ç»ƒé˜¶æ®µï¼Œæ·»åŠ æ ‡ç­¾ä¿¡æ¯å¸®åŠ©å­¦ä¹ 
        if include_labels and 'role_type' in event and 'sub_role_type' in event:
            role = event['role_type']
            sub_role = event['sub_role_type']
            desc = event['original_description']
            
            # å¼ºåˆ¶å¼•å¯¼ç”ŸæˆåŒ…å«èº«ä»½çš„ Pattern Memory
            # æˆ‘ä»¬å°†èº«ä»½ä¿¡æ¯ç›´æ¥åµŒå…¥åˆ°æè¿°ä¸­ï¼Œä½¿å…¶æˆä¸º"ç‰¹å¾"çš„ä¸€éƒ¨åˆ†
            enhanced_content += f"""

[Historical Labeled Sample]
Ground Truth Label: Role={role}, Sub-role={sub_role}
Visual Context: Matches visual profile for {sub_role}.

[CRITICAL INSTRUCTION FOR MEMORY EXTRACTION]
1. For the Factual Memory (UserMemory): Embed BOTH the observation text and labels in ONE sentence. Use this template:
   "[Factual Memory] Time: {time_str} | Observation: {desc} | Ground Truth Label: Role={role}, Sub-role={sub_role}"
2. Never emit an empty factual memory containing only the label.
3. For the Pattern Memory: You MUST include the specific identity label. Format it exactly like this:
   "Identity: {sub_role} ({role}) | Visuals: {desc}" """
        else:
            enhanced_content += f"""

Note: Extract key visual features (clothes, colors), vehicle details, and behavioral patterns to aid future identity re-identification."""

        add_req = APIADDRequest(
            user_id=user_id,
            mem_cube_id=mem_cube_id,
            messages=[{"role": "user", "content": enhanced_content}],
            session_id=session_id,
            source="anker_security"
        )
        try:
            add_memories(add_req)
            
            iter_cost = time.time() - event_start
            accumulated += iter_cost
            
            if total <= 50 or i % 20 == 0 or i == total:
                avg_cost = accumulated / i
                remaining = max(total - i, 0) * avg_cost
                speed = (1 / avg_cost) if avg_cost > 0 else 0
                percent = (i / total * 100) if total else 100
                
                # è®¡ç®—é¢„è®¡å®Œæˆæ—¶é—´
                eta_timestamp = time.time() + remaining
                eta_str = datetime.fromtimestamp(eta_timestamp).strftime("%H:%M:%S")
                
                phase_prefix = ""
                if phase_info:
                    phase_prefix = f"[é˜¶æ®µ {phase_info['current_phase']}/{phase_info['total_phases']}] "
                
                log.info(
                    f"    {phase_prefix}[å†™å…¥] {i}/{total} ({percent:.1f}%) | "
                    f"æœ¬æ¬¡ {iter_cost:.2f}s | å¹³å‡ {avg_cost:.2f}s | "
                    f"é€Ÿç‡ {speed:.1f}æ¡/s | æœ¬æ‰¹å‰©ä½™ {remaining:.0f}s | ETA {eta_str}"
                )
        except Exception as e:
            log.error(f"    âŒ å†™å…¥ç¬¬ {i}/{total} æ¡äº‹ä»¶æ—¶å¤±è´¥: {e}")
            
    elapsed = time.time() - start_t
    log.info(f"    âœ… æ‰¹æ¬¡å†™å…¥å®Œæˆï¼Œå…± {total} æ¡ï¼Œè€—æ—¶ {elapsed:.1f}s")
    
    # æ·»åŠ å®Œæˆåï¼Œç­‰å¾…ä¸€å°æ®µæ—¶é—´è®© MemOS å¤„ç†è®°å¿†æå–
    time.sleep(2)
    
    # æ¸…ç†ç›¸åŒç»´åº¦çš„é‡å¤è®°å¿†
    if clean_duplicates:
        log.info(f"    ğŸ” æ£€æŸ¥å¹¶æ¸…ç†ç›¸åŒç»´åº¦çš„é‡å¤è®°å¿†...")
        clean_duplicate_dimension_memories(mem_cube_id, family_logger=log, memory_stats=memory_stats)
    
    return elapsed

def infer_event(event, user_id, mem_cube_id):
    """æ¨ç†å•ä¸ªäº‹ä»¶ï¼ˆåŒ…å«æ£€ç´¢è®°å¿†ï¼‰
    
    Returns:
        tuple: (role_type, sub_role_type, confidence, reasoning, retrieved_memories, prompt)
    """
    desc = event['original_description']
    ts = event['timestamp']
    period = get_time_period(ts)
    fmt_time = format_timestamp(ts)
    
    # å…ˆæ£€ç´¢ç›¸å…³è®°å¿†
    retrieved_memories = []
    few_shot_examples = []
    learned_prototypes = []
    
    try:
        # 1. æ£€ç´¢ç›¸ä¼¼äº‹ä»¶ï¼ˆç”¨äº Few-Shotï¼‰
        # ä½¿ç”¨æè¿°æ£€ç´¢å…·ä½“çš„å†å²äº‹ä»¶
        search_query = f"Similar event: {desc[:200]}"
        memories = naive_mem_cube.text_mem.search(
            query=search_query,
            user_name=mem_cube_id,
            top_k=15,  # å¢åŠ æ•°é‡ä»¥ç­›é€‰é«˜è´¨é‡æ ·æœ¬
        )
        
        for mem in memories:
            mem_text = getattr(mem, "memory", "")
            mem_id = getattr(mem, "id", "N/A")
            metadata = getattr(mem, "metadata", None)
            created_at = _get_metadata_value(metadata, "created_at") or "N/A"
            
            # åˆ†ç±»è®°å¿†ç±»å‹
            is_pattern = "[Pattern Memory]" in mem_text
            is_gt_sample = "Ground Truth Label" in mem_text or "[Historical Labeled Sample]" in mem_text
            
            # æ”¶é›† Few-Shot æ ·æœ¬ (å¿…é¡»åŒ…å« GT æ ‡ç­¾)
            if is_gt_sample:
                few_shot_examples.append(mem_text)
            
            # æ”¶é›†åŸå‹ (Pattern Memory)
            if is_pattern and ("Identity:" in mem_text or "Visuals:" in mem_text):
                learned_prototypes.append(mem_text)
            
            # è®°å½•æ£€ç´¢åˆ°çš„æ‰€æœ‰ç›¸å…³è®°å¿†ï¼ˆç”¨äºè°ƒè¯•å’Œåˆ†æï¼‰
            retrieved_memories.append({
                "memory_id": mem_id,
                "memory_text": mem_text[:500],  # æˆªæ–­å¤ªé•¿çš„è®°å¿†
                "created_at": created_at,
                "type": "Pattern" if is_pattern else ("GT Sample" if is_gt_sample else "Other")
            })
            
        # é™åˆ¶æ•°é‡
        few_shot_examples = few_shot_examples[:5]
        learned_prototypes = learned_prototypes[:5]
            
    except Exception as e:
        logger.warning(f"Failed to retrieve memories: {e}")
    
    # æ„å»ºå¢å¼º Prompt
    few_shot_block = ""
    if few_shot_examples:
        few_shot_block = "\n[Historical Similar Events (Few-Shot Examples)]\n" + "\n".join([f"{i+1}. {m}" for i, m in enumerate(few_shot_examples)])
        
    prototype_block = ""
    if learned_prototypes:
        prototype_block = "\n[Learned Identity Prototypes]\n" + "\n".join([f"- {p}" for p in learned_prototypes])

    query = f"""Analyze this home security event and classify the identity based on learned family patterns.
{prototype_block}
{few_shot_block}

Current Event:
- Time: {fmt_time} ({period})
- Description: {desc}

## Classification System
**role_type**: General Identity | Passerby | Staff | Suspicious Person | Unspecified | Non-Human
**sub_role_type**: 
- General Identity â†’ Family Member | Visitor | Other General Identity
- Passerby â†’ Passerby
- Staff â†’ Delivery Person | Police | Service Worker | Other Staff
- Suspicious Person â†’ Unauthorized Entry | Property Damage | Armed Person | Fighting | Other Suspicious Person
- Non-Human â†’ Vehicle Activity | Pet/Animal Activity | Environmental Change Only

âš ï¸ CRITICAL: "Family Member" and "Visitor" are sub_role_type ONLY, never use them as role_type.

## Key Guidance

**Location is critical:**
- Person exiting FROM or inside courtyard/residence â†’ likely Family Member (not Passerby)
- Person walking PAST outside with no interaction â†’ likely Passerby
- Interacting with residence (door, car, mailbox) â†’ Family Member or Visitor

**Identity signals:**
- Visual match with learned family patterns â†’ Family Member
- Uniform or delivery behavior â†’ Staff
- Forced entry, weapons, breaking things â†’ Suspicious Person

Output format:
role_type: [one of the 6 types above]
sub_role_type: [corresponding sub-type]
confidence: [High/Medium/Low]
reasoning: [Your analysis: location, interaction, visual match, and conclusion]
"""
    try:
        response = llm.generate([{"role": "user", "content": query}])
        role, sub, conf, reasoning = parse_result(response)
        # åå¤„ç†ï¼šä¿®æ­£å¸¸è§çš„åˆ†ç±»é”™è¯¯ï¼ˆä¼ å…¥æè¿°ç”¨äºåŸºäºä½ç½®çš„ä¿®æ­£ï¼‰
        role, sub = fix_classification_errors(role, sub, desc)
        return role, sub, conf, reasoning, retrieved_memories, query
    except Exception as e:
        logger.error(f"Inference failed: {e}")
        return "Unspecified", "Unspecified", "Low", "", retrieved_memories, query

def fix_classification_errors(role_type, sub_role_type, description=""):
    """ä¿®æ­£å¸¸è§çš„åˆ†ç±»å±‚çº§é”™è¯¯
    
    å¸¸è§é”™è¯¯ï¼š
    1. Family Member / Visitor è¢«é”™è¯¯åœ°ç”¨ä½œ role_type
    2. éœ€è¦ç¡®ä¿ Family Member/Visitor çš„ role_type æ˜¯ General Identity
    3. ä»é™¢å­/ä½å®…é‡Œå‡ºæ¥çš„äººè¢«è¯¯åˆ¤ä¸ºPasserby
    """
    desc_lower = description.lower()
    
    # ğŸš¨ å…³é”®ä¿®æ­£ï¼šåŸºäºä½ç½®è¯­ä¹‰çš„å¼ºåˆ¶ä¿®æ­£
    # å¦‚æœæè¿°åŒ…å«"ä»é™¢å­é‡Œå‡ºæ¥"ç­‰å…³é”®è¯ï¼Œä½†è¢«åˆ†ç±»ä¸ºPasserbyï¼Œå¼ºåˆ¶æ”¹ä¸ºFamily Member
    exit_phrases = [
        "out of the courtyard",
        "from the courtyard", 
        "exits residence",
        "walks out of",
        "exiting from",
        "leaves the residence",
        "from the residence"
    ]
    
    inside_phrases = [
        "in the courtyard",
        "in the property",
        "in a residential courtyard",
        "inside the courtyard"
    ]
    
    # æ£€æŸ¥æ˜¯å¦ä»å†…éƒ¨å‡ºæ¥æˆ–åœ¨å†…éƒ¨æ´»åŠ¨
    is_exiting = any(phrase in desc_lower for phrase in exit_phrases)
    is_inside = any(phrase in desc_lower for phrase in inside_phrases)
    
    if (is_exiting or is_inside) and role_type == "Passerby":
        logger.warning(f"âš ï¸ ä½ç½®ä¿®æ­£: '{desc_lower[:50]}...' åŒ…å«ä½å®…å†…éƒ¨æ´»åŠ¨ï¼ŒPasserby â†’ General Identity / Family Member")
        return "General Identity", "Family Member"
    
    # ä¿®æ­£Family Memberé”™è¯¯
    if role_type.lower() in ["family member", "family"]:
        logger.warning(f"âš ï¸ å±‚çº§ä¿®æ­£: role_type='Family Member' â†’ 'General Identity' / 'Family Member'")
        return "General Identity", "Family Member"
    
    # ä¿®æ­£Visitoré”™è¯¯  
    if role_type.lower() == "visitor":
        logger.warning(f"âš ï¸ å±‚çº§ä¿®æ­£: role_type='Visitor' â†’ 'General Identity' / 'Visitor'")
        return "General Identity", "Visitor"
    
    # ä¿®æ­£å…¶ä»–å¯èƒ½çš„sub_roleè¢«ç”¨ä½œrole_typeçš„æƒ…å†µ
    sub_role_values = {
        "delivery person": ("Staff", "Delivery Person"),
        "police": ("Staff", "Police"),
        "service worker": ("Staff", "Service Worker"),
        "government worker": ("Staff", "Government Worker"),
        "unauthorized entry": ("Suspicious Person", "Unauthorized Entry"),
        "property damage": ("Suspicious Person", "Property Damage"),
        "armed person": ("Suspicious Person", "Armed Person"),
        "fighting": ("Suspicious Person", "Fighting"),
    }
    
    role_lower = role_type.lower()
    if role_lower in sub_role_values:
        correct_role, correct_sub = sub_role_values[role_lower]
        logger.warning(f"âš ï¸ ä¿®æ­£é”™è¯¯: role_type='{role_type}' â†’ '{correct_role}' / '{correct_sub}'")
        return correct_role, correct_sub
    
    return role_type, sub_role_type

def parse_result(text):
    role = "Unspecified"
    sub = "Unspecified"
    conf = "Low"
    reason = ""
    
    rm = re.search(r'role_type:\s*\[?([^\]\n]+)\]?', text, re.IGNORECASE)
    if rm: role = rm.group(1).strip()
    
    sm = re.search(r'sub_role_type:\s*\[?([^\]\n]+)\]?', text, re.IGNORECASE)
    if sm: sub = sm.group(1).strip()
    
    cm = re.search(r'confidence:\s*\[?([^\]\n]+)\]?', text, re.IGNORECASE)
    if cm: conf = cm.group(1).strip()
    
    return role, sub, conf, text

def _partition_predictions(predictions, ground_truths):
    """å°†é¢„æµ‹ç»“æœæŒ‰æ­£ç¡®/é”™è¯¯ç±»å‹åˆ†ç»„"""
    partitions = {
        "correct": [],
        "wrong_role_type": [],
        "wrong_sub_role_type": [],
        "wrong_both": [],
        "total": 0,
    }
    
    for pred in predictions:
        vid = pred['video_path']
        gt = ground_truths.get(vid)
        if not gt:
            continue
        
        partitions["total"] += 1
        
        p_role = pred['predicted_role_type'].strip().lower()
        p_sub = pred['predicted_sub_role_type'].strip().lower()
        g_role = gt['role_type'].strip().lower()
        g_sub = gt['sub_role_type'].strip().lower()
        
        role_correct = (p_role == g_role)
        sub_correct = (p_sub == g_sub)
        
        comparison = {
            "video_path": vid,
            "timestamp": gt.get('timestamp', 'N/A'),
            "description": gt.get('original_description', 'N/A'),
            "ground_truth": {
                "role_type": gt['role_type'],
                "sub_role_type": gt['sub_role_type']
            },
            "predicted": {
                "role_type": pred['predicted_role_type'],
                "sub_role_type": pred['predicted_sub_role_type'],
                "confidence": pred.get('confidence', 'N/A')
            },
            "reasoning": pred.get('reasoning', 'N/A'),
            "retrieved_memories_count": len(pred.get('retrieved_memories', []))
        }
        
        if role_correct and sub_correct:
            partitions["correct"].append(comparison)
        elif not role_correct and not sub_correct:
            partitions["wrong_both"].append(comparison)
        elif not role_correct:
            partitions["wrong_role_type"].append(comparison)
        else:
            partitions["wrong_sub_role_type"].append(comparison)
    
    return partitions


def _build_analysis_summary(partitions):
    """åŸºäºåˆ†ç»„ç»“æœç”Ÿæˆæ·±åº¦åˆ†ææ‘˜è¦"""
    total = partitions["total"]
    correct = len(partitions["correct"])
    wrong_role = len(partitions["wrong_role_type"])
    wrong_sub = len(partitions["wrong_sub_role_type"])
    wrong_both = len(partitions["wrong_both"])
    
    all_wrong = partitions["wrong_role_type"] + partitions["wrong_sub_role_type"] + partitions["wrong_both"]
    role_related_errors = partitions["wrong_role_type"] + partitions["wrong_both"]
    sub_related_errors = partitions["wrong_sub_role_type"] + partitions["wrong_both"]
    
    avg_retrieved = sum(s['retrieved_memories_count'] for s in all_wrong) / len(all_wrong) if all_wrong else 0
    zero_retrieval = sum(1 for s in all_wrong if s['retrieved_memories_count'] == 0)
    min_retrieved = min((s['retrieved_memories_count'] for s in all_wrong), default=0)
    max_retrieved = max((s['retrieved_memories_count'] for s in all_wrong), default=0)
    zero_ratio = (zero_retrieval / len(all_wrong)) if all_wrong else 0
    
    def _extract_confusions(records, label):
        counter = defaultdict(int)
        for rec in records:
            gt = rec['ground_truth'].get(label, "Unknown")
            pred = rec['predicted'].get(label, "Unknown")
            if gt == pred:
                continue
            counter[(gt, pred)] += 1
        sorted_pairs = sorted(counter.items(), key=lambda x: x[1], reverse=True)[:5]
        return [
            {"from": pair[0], "to": pair[1], "count": count}
            for (pair, count) in sorted_pairs
        ]
    
    role_confusions = _extract_confusions(role_related_errors, "role_type")
    sub_confusions = _extract_confusions(sub_related_errors, "sub_role_type")
    
    # æå–é«˜é¢‘è§†è§‰å…³é”®è¯ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
    confusion_keywords = defaultdict(int)
    stop_words = {'the', 'a', 'an', 'in', 'on', 'at', 'of', 'with', 'and', 'to', 'is', 'are',
                  'wearing', 'dressed', 'scene', 'near', 'from', 'toward', 'into', 'person', 'people'}
    for err in all_wrong:
        desc = err['description']
        if not desc or desc == 'N/A':
            continue
        words = re.findall(r'[a-zA-Z]{3,}', desc.lower())
        chinese_tokens = re.findall(r'[\u4e00-\u9fff]{1,2}', desc)
        for w in words:
            if w not in stop_words:
                confusion_keywords[w] += 1
        for token in chinese_tokens:
            confusion_keywords[token] += 1
    
    top_keywords = sorted(confusion_keywords.items(), key=lambda x: x[1], reverse=True)[:5]
    if not top_keywords and all_wrong:
        top_keywords = [{"keyword": "æè¿°ä¸è¶³ï¼Œæ— æ³•æå–è§†è§‰æ¨¡å¼", "count": len(all_wrong)}]
    elif not top_keywords:
        top_keywords = [{"keyword": "å…¨éƒ¨é¢„æµ‹æ­£ç¡®ï¼Œæ— è§†è§‰æ··æ·†", "count": 0}]
    
    suggestions = []
    
    if not all_wrong:
        suggestions.append("éªŒè¯é›†ä¸­æ ·æœ¬å…¨éƒ¨é¢„æµ‹æ­£ç¡®ï¼Œå¯å¢åŠ å›°éš¾æ ·æœ¬éªŒè¯æ³›åŒ–ã€‚")
    else:
        role_error_ratio = len(role_related_errors) / len(all_wrong)
        sub_error_ratio = len(sub_related_errors) / len(all_wrong)
        
        if zero_ratio > 0.3:
            suggestions.append("æ£€ç´¢å¬å›ä¸è¶³ï¼šè¶…è¿‡30%çš„é”™è¯¯æ ·æœ¬æœªæ£€ç´¢åˆ°è®°å¿†ï¼Œéœ€æ£€æŸ¥å‘é‡ç´¢å¼•æˆ–å¢å¤§ top_kã€‚")
        if avg_retrieved > 3 and len(partitions["wrong_both"]) / len(all_wrong) > 0.5:
            suggestions.append("æ¨ç†ä¸€è‡´æ€§è¾ƒå·®ï¼šæ£€ç´¢æ•°é‡å……è¶³ä½†ä»å¤§é‡é”™è¯¯ï¼Œå»ºè®®ä¼˜åŒ– Prompt æˆ–æå‡ Pattern Memory è´¨é‡ã€‚")
        if role_error_ratio >= 0.4:
            top_pair = role_confusions[0] if role_confusions else None
            detail = f"Top: {top_pair['from']}â†’{top_pair['to']}" if top_pair else "Top: N/A"
            suggestions.append(f"èº«ä»½å¤§ç±»æ··æ·†å æ¯”é«˜ï¼ˆ{role_error_ratio*100:.1f}%ï¼‰ã€‚{detail}")
        if sub_error_ratio >= 0.4:
            top_pair = sub_confusions[0] if sub_confusions else None
            detail = f"Top: {top_pair['from']}â†’{top_pair['to']}" if top_pair else "Top: N/A"
            suggestions.append(f"èº«ä»½å­ç±»æ··æ·†å æ¯”é«˜ï¼ˆ{sub_error_ratio*100:.1f}%ï¼‰ã€‚{detail}")
        def _kw_label(item):
            if isinstance(item, dict):
                return item.get("keyword", "")
            return item[0]
        
        if any(_kw_label(k) in {"black", "dark", "blue", "white"} for k in top_keywords):
            suggestions.append("è§†è§‰ç‰¹å¾è¿‡äºä¾èµ–é¢œè‰²ï¼Œå»ºè®®åœ¨æè¿°ä¸­åŠ å…¥æœé¥°ç»†èŠ‚æˆ–ç‰©å“ç‰¹å¾ä»¥å¢å¼ºåŒºåˆ†åº¦ã€‚")
        if not suggestions:
            suggestions.append("é”™è¯¯æ ·æœ¬æ•°é‡æœ‰é™ï¼Œå»ºè®®æ‰©å±•éªŒè¯é›†æˆ–ä¸»åŠ¨æ ‡æ³¨ä»¥æš´éœ²æ›´å¤šå¤±è´¥æ¨¡å¼ã€‚")
    
    summary = {
        "overview": {
            "total": total,
            "correct": correct,
            "accuracy": (correct / total) if total else 0,
            "wrong_role_only": wrong_role,
            "wrong_sub_only": wrong_sub,
            "wrong_both": wrong_both,
        },
        "retrieval_quality": {
            "avg_retrieved_for_errors": avg_retrieved,
            "min_retrieved_for_errors": min_retrieved,
            "max_retrieved_for_errors": max_retrieved,
            "zero_retrieval_ratio": zero_ratio,
        },
        "visual_confusion_keywords": [
            item if isinstance(item, dict) else {"keyword": item[0], "count": item[1]}
            for item in top_keywords
        ],
        "dominant_confusions": {
            "role_type": role_confusions,
            "sub_role_type": sub_confusions,
        },
        "suggestions": suggestions,
    }
    
    return summary


def analyze_evaluation_results(all_eval_results, family_id, output_dir, family_logger=None):
    """åˆ†ææ‰€æœ‰è¯„ä¼°ç»“æœï¼Œæ‰¾å‡ºæŠ½å–å¥½çš„å’Œä¸å¥½çš„æ ·æœ¬"""
    log = family_logger if family_logger else logger
    
    log.info(f"\n{'='*80}")
    log.info(f"ğŸ“Š æœ€ç»ˆè¯„ä¼°åˆ†æ - å®¶åº­ {family_id}")
    log.info(f"{'='*80}")
    
    # æ”¶é›†æ‰€æœ‰é¢„æµ‹ç»“æœï¼ˆä½¿ç”¨æœ€åä¸€æ¬¡è¯„ä¼°ï¼‰
    if not all_eval_results:
        log.warning("  âš ï¸  æ²¡æœ‰è¯„ä¼°ç»“æœå¯ä¾›åˆ†æ")
        return
    
    last_eval = all_eval_results[-1]
    predictions = last_eval.get("predictions", [])
    ground_truths = last_eval.get("ground_truths", {})
    
    partitions = _partition_predictions(predictions, ground_truths)
    summary = _build_analysis_summary(partitions)
    
    total = summary["overview"]["total"]
    correct_count = summary["overview"]["correct"]
    wrong_role_type = partitions["wrong_role_type"]
    wrong_sub_role_type = partitions["wrong_sub_role_type"]
    wrong_both = partitions["wrong_both"]
    correct_predictions = partitions["correct"]
    
    # è¾“å‡ºç»Ÿè®¡
    log.info(f"\n  âœ… å®Œå…¨æ­£ç¡®: {correct_count}/{total} ({correct_count/total*100:.1f}%)")
    log.info(f"  âŒ Role Type é”™è¯¯: {len(wrong_role_type)}")
    log.info(f"  âŒ Sub-role Type é”™è¯¯: {len(wrong_sub_role_type)}")
    log.info(f"  âŒ ä¸¤è€…éƒ½é”™è¯¯: {len(wrong_both)}")
    
    # åˆ†æå¸¸è§é”™è¯¯æ¨¡å¼
    log.info(f"\n  ğŸ” å¸¸è§é”™è¯¯æ¨¡å¼åˆ†æ:")
    
    # Role Type é”™è¯¯ç»Ÿè®¡
    if wrong_role_type or wrong_both:
        role_errors = wrong_role_type + wrong_both
        error_patterns = defaultdict(int)
        for err in role_errors:
            pattern = f"{err['ground_truth']['role_type']} â†’ {err['predicted']['role_type']}"
            error_patterns[pattern] += 1
        
        log.info(f"\n    Role Type æ··æ·†çŸ©é˜µ (Top 5):")
        for pattern, count in sorted(error_patterns.items(), key=lambda x: x[1], reverse=True)[:5]:
            log.info(f"      â€¢ {pattern}: {count}æ¬¡")
    
    # Sub-role Type é”™è¯¯ç»Ÿè®¡
    if wrong_sub_role_type or wrong_both:
        sub_errors = wrong_sub_role_type + wrong_both
        error_patterns = defaultdict(int)
        for err in sub_errors:
            pattern = f"{err['ground_truth']['sub_role_type']} â†’ {err['predicted']['sub_role_type']}"
            error_patterns[pattern] += 1
        
        log.info(f"\n    Sub-role Type æ··æ·†çŸ©é˜µ (Top 5):")
        for pattern, count in sorted(error_patterns.items(), key=lambda x: x[1], reverse=True)[:5]:
            log.info(f"      â€¢ {pattern}: {count}æ¬¡")

    log.info(f"\n  ğŸ§  æ·±åº¦é”™è¯¯å½’å› åˆ†æ:")
    log.info(f"    â€¢ é”™è¯¯æ ·æœ¬å¹³å‡æ£€ç´¢è®°å¿†æ•°: {summary['retrieval_quality']['avg_retrieved_for_errors']:.1f}")
    log.info(f"    â€¢ æ— è®°å¿†æ£€ç´¢æ ·æœ¬å æ¯”: {summary['retrieval_quality']['zero_retrieval_ratio']*100:.1f}%")
    if summary["visual_confusion_keywords"]:
        keywords_str = ", ".join([f"{item['keyword']}({item['count']})" for item in summary["visual_confusion_keywords"]])
        log.info(f"    â€¢ é«˜é¢‘æ··æ·†è§†è§‰ç‰¹å¾è¯: {keywords_str}")
    for sug in summary["suggestions"]:
        log.info(f"    âš ï¸  {sug}")

    # ä¿å­˜è¯¦ç»†åˆ†æ
    analysis_dir = output_dir / "analysis"
    analysis_dir.mkdir(parents=True, exist_ok=True)
    analysis_file = analysis_dir / f"{family_id}_final_analysis.json"
    
    analysis_data = {
        "family_id": family_id,
        "timestamp": datetime.now().isoformat(),
        "summary": summary["overview"],
        "deep_analysis": {
            "retrieval_quality": summary["retrieval_quality"],
            "visual_confusion_keywords": summary["visual_confusion_keywords"],
            "automated_suggestions": summary["suggestions"]
        },
        "correct_samples": correct_predictions[:10],  # ä¿å­˜å‰10ä¸ªæ­£ç¡®æ ·æœ¬
        "wrong_samples": {
            "wrong_role_type": wrong_role_type[:10],
            "wrong_sub_role_type": wrong_sub_role_type[:10],
            "wrong_both": wrong_both[:10]
        }
    }
    
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_data, f, indent=2, ensure_ascii=False)
    
    log.info(f"\n  ğŸ’¾ è¯¦ç»†åˆ†æå·²ä¿å­˜åˆ°: {analysis_file}")
    log.info(f"{'='*80}\n")


def evaluate_on_set(eval_events, user_id, mem_cube_id, desc="Validation", phase_info=None, 
                    family_id=None, output_dir=None, family_logger=None, save_details=True):
    """åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ˆå¹¶è¡Œæ¨ç†ï¼‰
    
    Args:
        phase_info: dict with keys: current_phase, total_phases
        family_id: å®¶åº­IDï¼ˆç”¨äºä¿å­˜è¯¦ç»†ç»“æœï¼‰
        output_dir: è¾“å‡ºç›®å½•
        family_logger: æ—¥å¿—è®°å½•å™¨
        save_details: æ˜¯å¦ä¿å­˜è¯¦ç»†ç»“æœ
    """
    log = family_logger if family_logger else logger
    log.info(f"  ğŸ” æ­£åœ¨è¿›è¡Œ {desc} è¯„ä¼° (å…± {len(eval_events)} æ¡ï¼Œå¹¶è¡Œæ¨ç†)...")
    
    ground_truths = {e['video_path']: e for e in eval_events}
    predictions = [None] * len(eval_events)
    
    start_t = time.time()
    total = len(eval_events)
    
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    def _eval_infer(idx_event):
        idx, event = idx_event
        role, sub, conf, reasoning, retrieved_memories, prompt = infer_event(event, user_id, mem_cube_id)
        return idx, {
            "video_path": event['video_path'],
            "predicted_role_type": role,
            "predicted_sub_role_type": sub,
            "confidence": conf,
            "reasoning": reasoning,
            "retrieved_memories": retrieved_memories,
            "prompt": prompt
        }
    
    # å¹¶è¡Œæ¨ç†ï¼Œæ— workeré™åˆ¶
    with ThreadPoolExecutor() as executor:
        futures = [executor.submit(_eval_infer, (idx, event)) for idx, event in enumerate(eval_events)]
        completed = 0
        
        for future in as_completed(futures):
            idx, pred = future.result()
            predictions[idx] = pred
            completed += 1
            
            # ä¼˜åŒ–è¿›åº¦æ˜¾ç¤º
            elapsed = time.time() - start_t
            avg_time = elapsed / completed if completed else 0
            remaining = (total - completed) * avg_time
            eta_timestamp = time.time() + remaining
            eta_str = datetime.fromtimestamp(eta_timestamp).strftime("%H:%M:%S")
            
            if completed % 5 == 0 or completed == total:  # æ¯5æ¡æ›´æ–°ä¸€æ¬¡
                bar_len = 20
                filled = int(completed / total * bar_len)
                bar = "â–ˆ" * filled + "â–‘" * (bar_len - filled)
                
                phase_prefix = ""
                if phase_info:
                    phase_prefix = f"[é˜¶æ®µ {phase_info['current_phase']}/{phase_info['total_phases']}] "
                
                sys.stdout.write(
                    f"\r    {phase_prefix}[è¯„ä¼°] [{bar}] {completed}/{total} | "
                    f"è€—æ—¶ {elapsed:.1f}s | å‰©ä½™ {remaining:.0f}s | ETA {eta_str}"
                )
                sys.stdout.flush()
    
    sys.stdout.write("\n")
    metrics = MetricsCalculator.calculate(predictions, ground_truths)
    partitions = _partition_predictions(predictions, ground_truths)
    analysis_summary = _build_analysis_summary(partitions)
    
    # è¾“å‡ºæ‰€æœ‰é”™è¯¯æ¡ˆä¾‹çš„æ¨ç†åŸå› ï¼ˆå®Œæ•´æè¿°ï¼Œä¸æˆªæ–­ï¼‰
    # æ”¶é›†æ‰€æœ‰é”™è¯¯æ ·æœ¬ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
    all_wrong = partitions["wrong_both"] + partitions["wrong_role_type"] + partitions["wrong_sub_role_type"]
    
    if all_wrong:
        log.info(f"\n  ğŸ” é”™è¯¯æ ·æœ¬æ¨ç†åˆ†æ (å…± {len(all_wrong)} ä¸ªé”™è¯¯):")
        # æ˜¾ç¤ºæ‰€æœ‰é”™è¯¯æ ·æœ¬ï¼ˆæ— é™åˆ¶ï¼‰
        for i, sample in enumerate(all_wrong, 1):
            log.info(f"    ã€é”™è¯¯æ ·æœ¬ {i}ã€‘")
            log.info(f"      æè¿°: {sample['description']}")  # å®Œæ•´æè¿°ï¼Œä¸æˆªæ–­
            log.info(f"      çœŸå®æ ‡ç­¾: {sample['ground_truth']['role_type']} / {sample['ground_truth']['sub_role_type']}")
            log.info(f"      é¢„æµ‹æ ‡ç­¾: {sample['predicted']['role_type']} / {sample['predicted']['sub_role_type']}")
            log.info(f"      ç½®ä¿¡åº¦: {sample['predicted']['confidence']}")
            log.info(f"      ä½¿ç”¨è®°å¿†æ•°: {sample['retrieved_memories_count']}")
            # æå–reasoningçš„æ ¸å¿ƒéƒ¨åˆ†ï¼ˆå»æ‰é‡å¤çš„æ ‡ç­¾ä¿¡æ¯ï¼‰
            reasoning_text = sample.get('reasoning', '')
            if 'reasoning:' in reasoning_text:
                reasoning_core = reasoning_text.split('reasoning:')[-1].strip()
                # å®Œæ•´æ˜¾ç¤ºæ¨ç†ä¾æ®ï¼Œä¸æˆªæ–­
                log.info(f"      æ¨ç†ä¾æ®: {reasoning_core}")
            log.info("")
    
    if phase_info and phase_info.get('current_phase') == phase_info.get('total_phases'):
        log.info(f"  ğŸ§  æœ€ç»ˆé˜¶æ®µé”™è¯¯åˆ†æ:")
        log.info(f"    - å¹³å‡æ£€ç´¢è®°å¿†æ•°(é”™è¯¯æ ·æœ¬): {analysis_summary['retrieval_quality']['avg_retrieved_for_errors']:.1f}")
        log.info(f"    - æ— æ£€ç´¢å‘½ä¸­å æ¯”: {analysis_summary['retrieval_quality']['zero_retrieval_ratio']*100:.1f}%")
        if analysis_summary["visual_confusion_keywords"]:
            keywords_str = ", ".join([f"{item['keyword']}({item['count']})" for item in analysis_summary["visual_confusion_keywords"]])
            log.info(f"    - é«˜é¢‘è§†è§‰æ··æ·†ç‰¹å¾: {keywords_str}")
        for sug in analysis_summary["suggestions"]:
            log.info(f"    âš ï¸  {sug}")
    
    log.info(f"  ğŸ“Š è¯„ä¼°ç»“æœ ({desc}):")
    log.info(f"    - èº«ä»½å¤§ç±»å‡†ç¡®ç‡: {metrics['role_acc']*100:.1f}%")
    log.info(f"    - èº«ä»½å­ç±»å‡†ç¡®ç‡: {metrics['sub_role_acc']*100:.1f}%")
    log.info(f"    - å®¶äººè¯†åˆ«å‡†ç¡®ç‡: {metrics['family_acc']*100:.1f}%")
    log.info(f"    - å¼‚å¸¸æ£€æµ‹å‡†ç¡®ç‡: {metrics['anomaly_acc']*100:.1f}%")
    
    # ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœ
    if save_details and family_id and output_dir:
        eval_dir = output_dir / "evaluations"
        eval_dir.mkdir(parents=True, exist_ok=True)
        
        # æ„å»ºæ–‡ä»¶å
        phase_str = f"phase_{phase_info['current_phase']}" if phase_info else "final"
        eval_file = eval_dir / f"{family_id}_{phase_str}_eval.json"
        
        # æ„å»ºè¯¦ç»†å¯¹æ¯”
        detailed_results = []
        for pred in predictions:
            vid = pred['video_path']
            if vid in ground_truths:
                gt = ground_truths[vid]
                detailed_results.append({
                    "video_path": vid,
                    "timestamp": gt.get('timestamp', 'N/A'),
                    "description": gt.get('original_description', 'N/A'),
                    "ground_truth": {
                        "role_type": gt['role_type'],
                        "sub_role_type": gt['sub_role_type']
                    },
                    "predicted": {
                        "role_type": pred['predicted_role_type'],
                        "sub_role_type": pred['predicted_sub_role_type'],
                        "confidence": pred['confidence']
                    },
                    "correct": {
                        "role_type": pred['predicted_role_type'].strip().lower() == gt['role_type'].strip().lower(),
                        "sub_role_type": pred['predicted_sub_role_type'].strip().lower() == gt['sub_role_type'].strip().lower()
                    },
                    "reasoning": pred.get('reasoning', 'N/A'),
                    "retrieved_memories": pred.get('retrieved_memories', []),
                    "prompt": pred.get('prompt', 'N/A')
                })
        
        eval_data = {
            "family_id": family_id,
            "phase": phase_str,
            "description": desc,
            "timestamp": datetime.now().isoformat(),
            "metrics": metrics,
            "total_samples": total,
            "detailed_results": detailed_results,
            "analysis_summary": analysis_summary
        }
        
        with open(eval_file, 'w', encoding='utf-8') as f:
            json.dump(eval_data, f, indent=2, ensure_ascii=False)
        
        log.info(f"  ğŸ’¾ è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {eval_file}")
    
    return metrics, predictions, ground_truths

def process_family(
    family_id: str,
    output_dir: Path,
    split_ratio=0.1,
    max_train_events: int | None = None,
    max_test_samples: int | None = None,
    preflight: bool = False,
    family_logger=None,
):
    """å¤„ç†å•ä¸ªå®¶åº­çš„å®Œæ•´æµç¨‹"""
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥family_loggerï¼Œä½¿ç”¨å…¨å±€logger
    if family_logger is None:
        family_logger = logger
    
    # æ³¨æ„ï¼šä¸è¦ä¿®æ”¹å…¨å±€loggerï¼Œç›´æ¥ä½¿ç”¨family_logger
    
    family_logger.info(f"\n{'#'*60}")
    family_logger.info(f"ğŸ  å¤„ç†å®¶åº­: {family_id}")
    family_logger.info(f"{'#'*60}")
    
    # 1. å‡†å¤‡ç¯å¢ƒ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    user_id = f"anker_{family_id}_{timestamp}"
    mem_cube_id = f"cube_{family_id}_{timestamp}"
    session_id = f"sess_{family_id}_{timestamp}"
    
    # 2. åŠ è½½å¹¶åˆ‡åˆ†æ•°æ®
    train_all, test_oct = load_data(family_id)
    
    if max_train_events is not None:
        train_all = train_all[:max_train_events]
    if max_test_samples is not None:
        test_oct = test_oct[:max_test_samples]
    
    # éšæœºåˆ‡åˆ† Validation Set (å›ºå®š50æ¡ï¼Œä»9æœˆæ•°æ®ä¸­éšæœºé‡‡æ ·)
    val_size = 50  # å›ºå®šéªŒè¯é›†å¤§å°ä¸º50æ¡
    # ç¡®ä¿æ•°æ®é‡è¶³å¤Ÿ
    if len(train_all) < val_size + 10:  # è‡³å°‘éœ€è¦50æ¡éªŒè¯+10æ¡è®­ç»ƒ
        # å¦‚æœæ•°æ®å¤ªå°‘ï¼ŒåŠ¨æ€è°ƒæ•´éªŒè¯é›†å¤§å°
        val_size = max(3, int(len(train_all) * 0.2))  # è‡³å°‘3æ¡ï¼Œæœ€å¤š20%
    
    # éšæœºé‡‡æ ·éªŒè¯é›†ç´¢å¼•
    all_indices = list(range(len(train_all)))
    random.seed(42)  # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°
    val_indices = set(random.sample(all_indices, val_size))
    
    # åˆ†ç¦»è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆç¡®ä¿æ— äº¤é›†ï¼‰
    validation_set = [train_all[i] for i in range(len(train_all)) if i in val_indices]
    train_stream = [train_all[i] for i in range(len(train_all)) if i not in val_indices]
    
    # 3. æ¸è¿›å¼è®­ç»ƒä¸éªŒè¯
    # split_ratio æ§åˆ¶è®­ç»ƒæµæ¯æ¬¡åŠ å…¥çš„æ¯”ä¾‹ï¼ˆé»˜è®¤ 20%ï¼‰
    chunk_size = int(len(train_stream) * split_ratio)
    # ç¡®ä¿chunkä¸ä¸º0
    chunk_size = max(chunk_size, 1)
    total_phases = math.ceil(len(train_stream) / chunk_size) if len(train_stream) > 0 else 0
    
    mode_label = "PRE-FLIGHT" if preflight else "FULL RUN"
    family_logger.info(f"ğŸ“Š æ•°æ®æ¦‚è§ˆ ({mode_label}):")
    family_logger.info(f"  - 9æœˆæ€»æ•°æ®: {len(train_all)}")
    family_logger.info(f"  - è®­ç»ƒæµ (Training Stream): {len(train_stream)} (éšæœºé‡‡æ ·)")
    family_logger.info(f"  - éªŒè¯é›† (Validation Set): {len(validation_set)} (éšæœºé‡‡æ ·ï¼Œæ— äº¤é›†)")
    family_logger.info(f"  - 10æœˆæµ‹è¯•é›†: {len(test_oct)}")
    family_logger.info(f"  - è®­ç»ƒé˜¶æ®µæ•°: {total_phases} (chunk size={chunk_size})")
    
    family_logger.info(f"\nğŸ”„ å¼€å§‹æ¸è¿›å¼å­¦ä¹  ({mode_label}) (Chunk size: {chunk_size})...")
    
    progress_log = []
    all_eval_results = []  # ä¿å­˜æ‰€æœ‰è¯„ä¼°ç»“æœç”¨äºæœ€ç»ˆåˆ†æ
    phase_start_time = time.time()
    
    # åˆå§‹åŒ–è®°å¿†æ“ä½œç»Ÿè®¡
    memory_stats = {"deletion_ops": 0, "deleted_count": 0}
    
    # é¢„ä¼°æ¯ä¸ªé˜¶æ®µçš„æ—¶é—´ï¼ˆåŸºäºé¦–ä¸ªé˜¶æ®µçš„å®é™…è€—æ—¶åŠ¨æ€è°ƒæ•´ï¼‰
    estimated_phase_time = None
    
    current_idx = 0
    chunk_num = 1
    
    while current_idx < len(train_stream):
        end_idx = min(current_idx + chunk_size, len(train_stream))
        current_chunk = train_stream[current_idx:end_idx]
        
        phase_info = {"current_phase": chunk_num, "total_phases": total_phases, "phase_start_time": phase_start_time}
        
        family_logger.info(f"\nğŸ‘‰ ç¬¬ {chunk_num}/{total_phases} é˜¶æ®µ: åŠ å…¥ {len(current_chunk)} æ¡è®°å¿† ({current_idx}-{end_idx}) [{mode_label}]")
        family_logger.info(f"   ğŸ“ˆ å®¶åº­ {family_id} è®­ç»ƒè¿›åº¦: {end_idx}/{len(train_stream)} ({end_idx/len(train_stream)*100:.1f}%)")
        
        phase_iter_start = time.time()
        
        # A. åŠ å…¥è®°å¿†ï¼ˆåŒ…å«æ ‡ç­¾ä¿¡æ¯ç”¨äºå­¦ä¹ ï¼Œå¹¶è‡ªåŠ¨æ¸…ç†é‡å¤ç»´åº¦ï¼‰
        add_events_to_memory(current_chunk, user_id, mem_cube_id, session_id, 
                            phase_info=phase_info, include_labels=True,
                            clean_duplicates=True, family_logger=family_logger,
                            memory_stats=memory_stats)
        
        # B. éªŒè¯
        family_logger.info(f"   ğŸ§ª åœ¨éªŒè¯é›†ä¸Šè¯„ä¼° (åŸºäºå‰ {end_idx} æ¡è®°å¿†)...")
        metrics, predictions, ground_truths = evaluate_on_set(
            validation_set, user_id, mem_cube_id, f"Phase {chunk_num}", 
            phase_info=phase_info,
            family_id=family_id,
            output_dir=output_dir,
            family_logger=family_logger,
            save_details=True
        )
        
        # ä¿å­˜è¯„ä¼°ç»“æœç”¨äºæœ€ç»ˆåˆ†æ
        all_eval_results.append({
            "phase": chunk_num,
            "predictions": predictions,
            "ground_truths": ground_truths,
            "metrics": metrics
        })
        
        # è®°å½•å•é˜¶æ®µè€—æ—¶å¹¶é¢„ä¼°å‰©ä½™æ—¶é—´
        phase_elapsed = time.time() - phase_iter_start
        if estimated_phase_time is None:
            estimated_phase_time = phase_elapsed  # é¦–æ¬¡è®°å½•
        else:
            # åŠ¨æ€å¹³å‡
            estimated_phase_time = (estimated_phase_time + phase_elapsed) / 2
        
        remaining_phases = total_phases - chunk_num
        estimated_remaining = remaining_phases * estimated_phase_time
        total_eta_timestamp = time.time() + estimated_remaining
        total_eta_str = datetime.fromtimestamp(total_eta_timestamp).strftime("%H:%M:%S")
        
        family_logger.info(
            f"   â±ï¸  æœ¬é˜¶æ®µè€—æ—¶: {phase_elapsed:.1f}s | "
            f"å‰©ä½™ {remaining_phases} é˜¶æ®µ | é¢„è®¡å®Œæˆæ—¶é—´: {total_eta_str}"
        )
        
        progress_log.append({
            "phase": chunk_num,
            "trained_events": end_idx,
            "metrics": metrics
        })
        
        current_idx = end_idx
        chunk_num += 1
    
    # ç»˜åˆ¶å‡†ç¡®ç‡è¿›åº¦æ›²çº¿
    plot_progress_metrics(progress_log, family_id, output_dir, mode_label, family_logger=family_logger)
        
    # æœ€ç»ˆåˆ†æè¯„ä¼°ç»“æœï¼ˆä»…åœ¨éé¢„æ£€æ¨¡å¼ä¸‹æ‰§è¡Œï¼‰
    if not preflight and all_eval_results:
        analyze_evaluation_results(all_eval_results, family_id, output_dir, family_logger)
        
    # 4. è¡¥å……éªŒè¯é›†åˆ°è®°å¿†ï¼ˆåŒ…å«æ ‡ç­¾ï¼‰
    family_logger.info(f"\nğŸ“¦ è¡¥å……éªŒè¯é›†åˆ°è®°å¿†åº“ (ä¸ºäº†10æœˆæ¨ç†)...")
    add_events_to_memory(validation_set, user_id, mem_cube_id, session_id, 
                        phase_info=None, include_labels=True,
                        clean_duplicates=True, family_logger=family_logger,
                        memory_stats=memory_stats)
    
    # ä¿å­˜æœ€ç»ˆçš„å…¨é‡è®°å¿†å’Œç»Ÿè®¡
    save_all_memories_and_stats(family_id, mem_cube_id, output_dir, family_logger, memory_stats)
    
    # 5. æœ€ç»ˆ10æœˆæ¨ç†ï¼ˆå®æ—¶å†™å…¥ï¼‰
    family_logger.info(f"\nğŸ”® å¼€å§‹10æœˆæ•°æ®å…¨é‡æ¨ç† ({len(test_oct)} æ¡) [{mode_label}]...")
    
    suffix = "preflight" if preflight else "full_results"
    out_file = output_dir / f"{family_id}_{suffix}.json"
    
    # åˆå§‹åŒ–æ–‡ä»¶ï¼Œå†™å…¥åŸºç¡€ç»“æ„
    initial_data = {
        "family_id": family_id,
        "progress_metrics": progress_log,
        "october_predictions": [],
        "status": "in_progress",
        "total": len(test_oct),
        "completed": 0
    }
    with open(out_file, 'w', encoding='utf-8') as f:
        json.dump(initial_data, f, indent=2, ensure_ascii=False)
    
    oct_predictions = [None] * len(test_oct)
    start_t = time.time()
    completed_count = 0
    
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import threading
    write_lock = threading.Lock()
    
    def _infer(idx_event):
        idx, event = idx_event
        role, sub, conf, reasoning, retrieved_memories, prompt = infer_event(event, user_id, mem_cube_id)
        return idx, {
            "video_path": event['video_path'],
            "timestamp": event['timestamp'],
            "original_description": event['original_description'],
            "predicted_role_type": role,
            "predicted_sub_role_type": sub,
            "confidence": conf,
            "reasoning": reasoning,
            "retrieved_memories": retrieved_memories,
            "prompt": prompt
        }
    
    with ThreadPoolExecutor() as executor:
        futures = [executor.submit(_infer, (idx, event)) for idx, event in enumerate(test_oct)]
        total = len(futures)
        
        for future in as_completed(futures):
            idx, pred = future.result()
            oct_predictions[idx] = pred
            completed_count += 1
            
            # å®æ—¶å†™å…¥æ–‡ä»¶ï¼ˆæ¯å®Œæˆä¸€æ¡å°±æ›´æ–°ï¼‰
            with write_lock:
                current_data = {
                    "family_id": family_id,
                    "progress_metrics": progress_log,
                    "october_predictions": [p for p in oct_predictions if p is not None],
                    "status": "in_progress" if completed_count < total else "completed",
                    "total": total,
                    "completed": completed_count
                }
                with open(out_file, 'w', encoding='utf-8') as f:
                    json.dump(current_data, f, indent=2, ensure_ascii=False)
            
            if completed_count % 5 == 0 or completed_count == total:
                percent = completed_count / total * 100 if total else 100
                elapsed = time.time() - start_t
                avg = elapsed / completed_count if completed_count else 0
                remaining = (total - completed_count) * avg
                eta_timestamp = time.time() + remaining
                eta_str = datetime.fromtimestamp(eta_timestamp).strftime("%H:%M:%S")
                logger.info(
                    f"    [10æœˆæ¨ç†] {completed_count}/{total} ({percent:.1f}%) | "
                    f"è€—æ—¶ {elapsed:.1f}s | å‰©ä½™ {remaining:.0f}s | ETA {eta_str}"
                )
    
    total_time = time.time() - start_t
    family_logger.info(f"\nâœ… 10æœˆæ¨ç†å®Œæˆ! å¹¶å‘è€—æ—¶: {total_time:.1f}s")
    family_logger.info(f"ğŸ’¾ ç»“æœå®æ—¶ä¿å­˜è‡³: {out_file}")
    
    # æœ€ç»ˆæ•°æ®
    final_data = {
        "family_id": family_id,
        "progress_metrics": progress_log,
        "october_predictions": oct_predictions,
        "status": "completed",
        "total": len(test_oct),
        "completed": len([p for p in oct_predictions if p is not None])
    }
    
    return final_data

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Ankerå®¶åº­å®‰é˜²æ¸è¿›å¼æ¨ç†")
    parser.add_argument(
        "--mode",
        choices=["quick", "single", "all", "fast_test"],
        default="single",
        help="è¿è¡Œæ¨¡å¼ï¼šquick=æœ€å°æ•°æ®å®¶åº­ï¼Œsingle=å•ä¸ªå®¶åº­ï¼Œall=å…¨éƒ¨å®¶åº­ï¼Œfast_test=å¿«é€Ÿå…¨æµç¨‹éªŒè¯(è®­ç»ƒ200+éªŒè¯50+æ¨ç†10)",
    )
    parser.add_argument(
        "--family",
        type=str,
        default=None,
        help="æŒ‡å®šå®¶åº­IDï¼ˆsingleæ¨¡å¼å¿…å¡«ï¼Œallæ¨¡å¼å¯é€‰ï¼‰",
    )
    parser.add_argument(
        "--max-train-events",
        type=int,
        default=None,
        help="é™åˆ¶è®­ç»ƒäº‹ä»¶æ•°é‡ï¼ˆå¯é€‰ï¼‰",
    )
    parser.add_argument(
        "--max-test-samples",
        type=int,
        default=None,
        help="é™åˆ¶10æœˆæ¨ç†äº‹ä»¶æ•°é‡ï¼ˆå¯é€‰ï¼‰",
    )
    parser.add_argument(
        "--split-ratio",
        type=float,
        default=0.26,
        help="è®­ç»ƒæµæ¯é˜¶æ®µæ¯”ä¾‹ï¼ˆé»˜è®¤0.26=26%ï¼Œç¡®ä¿æœ€å¤š4ä¸ªé˜¶æ®µï¼‰",
    )
    
    args = parser.parse_args()
    
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = project_root / "examples" / "poc" / "inference_results" / f"progressive_{ts}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"ğŸš€ ä»»åŠ¡å¼€å§‹ - æ¨¡å¼: {args.mode} - è¾“å‡ºç›®å½•: {output_dir}")
    
    summary = {}
    
    def run_preflight_check(target_family: str):
        logger.info(f"ğŸ§ª é¢„æ£€: åœ¨å®¶åº­ {target_family} ä¸Šæ‰§è¡Œå°æ ·æœ¬æµç¨‹ä»¥éªŒè¯ç®¡çº¿...")
        preflight_dir = output_dir / "preflight_checks"
        preflight_dir.mkdir(exist_ok=True)
        process_family(
            target_family,
            preflight_dir,
            split_ratio=0.5,  # é¢„æ£€é˜¶æ®µä½¿ç”¨50%ï¼Œåªéœ€2ä¸ªé˜¶æ®µ
            max_train_events=15,  # 15æ¡æ•°æ® â†’ è®­ç»ƒæµ12æ¡+éªŒè¯é›†3æ¡ â†’ 2ä¸ªé˜¶æ®µ(æ¯é˜¶æ®µ6æ¡)
            max_test_samples=3,   # åªæ¨ç†3æ¡æµ‹è¯•æ•°æ®
            preflight=True,
        )
        logger.info("ğŸ§ª é¢„æ£€å®Œæˆï¼Œå¼€å§‹æ­£å¼æµç¨‹...\n")
    
    if args.mode == "fast_test":
        # å¿«é€Ÿå…¨æµç¨‹éªŒè¯ï¼šæ‰€æœ‰å®¶åº­ï¼Œè®­ç»ƒ200æ¡+éªŒè¯50æ¡ï¼Œæ¨ç†10æ¡
        families = FAMILY_ORDER if not args.family else [args.family]
        total = len(families)
        logger.info(f"âš¡ å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šå¹¶è¡Œæµ‹è¯•æ‰€æœ‰å®¶åº­ ({total}ä¸ª) - è®­ç»ƒ200æ¡+éªŒè¯50æ¡+æ¨ç†10æ¡")
        
        # ä¸ºæ¯ä¸ªå®¶åº­åˆ›å»ºç‹¬ç«‹çš„æ—¥å¿—æ–‡ä»¶
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import threading
        
        family_loggers = {}
        log_dir = output_dir / "family_logs"
        log_dir.mkdir(exist_ok=True)
        
        def setup_family_logger(family_id):
            """ä¸ºæ¯ä¸ªå®¶åº­åˆ›å»ºç‹¬ç«‹çš„ logger"""
            family_logger = logging.getLogger(f"family_{family_id}")
            family_logger.setLevel(logging.INFO)
            family_logger.handlers.clear()
            family_logger.propagate = False  # é˜²æ­¢æ—¥å¿—å‘ä¸Šä¼ æ’­åˆ°æ ¹logger
            
            # æ–‡ä»¶handler
            fh = logging.FileHandler(log_dir / f"{family_id}.log", encoding='utf-8', mode='w')
            fh.setLevel(logging.INFO)
            formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')
            fh.setFormatter(formatter)
            family_logger.addHandler(fh)
            
            # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°ï¼ˆå¸¦å®¶åº­IDå‰ç¼€ï¼‰
            ch = logging.StreamHandler()
            ch.setLevel(logging.INFO)
            ch_formatter = logging.Formatter(f'[{family_id}] %(message)s')
            ch.setFormatter(ch_formatter)
            family_logger.addHandler(ch)
            
            return family_logger
        
        for fam in families:
            family_loggers[fam] = setup_family_logger(fam)
        
        def process_family_fast_test(family_id):
            """å¿«é€Ÿæµ‹è¯•å•ä¸ªå®¶åº­"""
            family_logger = family_loggers[family_id]
            try:
                family_logger.info(f"â–¶ å¿«é€Ÿæµ‹è¯•å®¶åº­ {family_id}")
                
                process_family(
                    family_id,
                    output_dir,
                    split_ratio=0.25,  # æ¯æ¬¡åŠ å…¥25%ï¼Œ4ä¸ªæ‰¹æ¬¡
                    max_train_events=250,  # å¿«é€Ÿæµ‹è¯•ï¼š250æ¡è®­ç»ƒæ•°æ®ï¼ˆè®­ç»ƒæµ200æ¡+éªŒè¯é›†50æ¡ï¼‰â†’ 4æ‰¹æ¬¡ï¼Œæ¯æ‰¹50æ¡
                    max_test_samples=10,   # å¿«é€Ÿæµ‹è¯•ï¼š10æ¡10æœˆæ•°æ®
                    preflight=False,
                    family_logger=family_logger,
                )
                
                family_logger.info(f"âœ… å®Œæˆå®¶åº­ {family_id}")
                return family_id, "Success", None
            except Exception as e:
                family_logger.error(f"âŒ å®¶åº­ {family_id} å¤„ç†å¤±è´¥: {e}")
                import traceback
                family_logger.error(traceback.format_exc())
                return family_id, "Failed", str(e)
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰å®¶åº­
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¿«é€Ÿæµ‹è¯• {total} ä¸ªå®¶åº­...")
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=min(total, 3)) as executor:
            futures = {executor.submit(process_family_fast_test, fam): fam for fam in families}
            completed = 0
            
            for future in as_completed(futures):
                family_id, status, error = future.result()
                completed += 1
                summary[family_id] = status if status == "Success" else f"Failed: {error}"
                
                elapsed = time.time() - start_time
                logger.info(
                    f"ğŸ“Š è¿›åº¦: {completed}/{total} ({completed/total*100:.1f}%) | "
                    f"å·²å®Œæˆ: {family_id} ({status}) | æ€»è€—æ—¶: {elapsed:.1f}s"
                )
        
        total_elapsed = time.time() - start_time
        logger.info(f"\nğŸ‰ å¿«é€Ÿæµ‹è¯•å®Œæˆï¼æ€»è€—æ—¶: {total_elapsed:.1f}s")
    
    elif args.mode == "quick":
        target_family = args.family or FAMILY_ORDER[0]
        logger.info(f"ğŸ”¹ å¿«é€Ÿæ¨¡å¼ï¼šä»…å¤„ç†æ•°æ®é‡æœ€å°çš„å®¶åº­ {target_family}")
        process_family(
            target_family,
            output_dir,
            split_ratio=args.split_ratio,
            max_train_events=args.max_train_events or 10,
            max_test_samples=args.max_test_samples or 10,
        )
        summary[target_family] = "Success"
    
    elif args.mode == "single":
        if not args.family:
            args.family = FAMILY_ORDER[0]  # é»˜è®¤ä½¿ç”¨æ•°æ®é‡æœ€å°çš„å®¶åº­
        logger.info(f"ğŸ”¹ å•å®¶åº­æ¨¡å¼ï¼š{args.family}")
        run_preflight_check(args.family)
        process_family(
            args.family,
            output_dir,
            split_ratio=args.split_ratio,
            max_train_events=args.max_train_events,
            max_test_samples=args.max_test_samples,
            preflight=False,
        )
        summary[args.family] = "Success"
    
    else:  # å…¨é‡æ¨¡å¼ - å¹¶è¡Œå¤„ç†æ‰€æœ‰å®¶åº­
        families = FAMILY_ORDER if not args.family else [args.family]
        total = len(families)
        
        logger.info("ğŸ§ª å…¨é‡æ¨¡å¼é¢„æ£€ï¼šå°†å¹¶è¡Œå¯¹æ¯ä¸ªå®¶åº­æ‰§è¡Œå°æ ·æœ¬æµç¨‹...")
        
        # å¹¶è¡Œæ‰§è¡Œé¢„æ£€
        from concurrent.futures import ThreadPoolExecutor, as_completed
        with ThreadPoolExecutor(max_workers=min(total, 3)) as executor:
            preflight_futures = {executor.submit(run_preflight_check, fam): fam for fam in families}
            for future in as_completed(preflight_futures):
                fam = preflight_futures[future]
                try:
                    future.result()
                    logger.info(f"âœ… å®¶åº­ {fam} é¢„æ£€å®Œæˆ")
                except Exception as e:
                    logger.error(f"âŒ å®¶åº­ {fam} é¢„æ£€å¤±è´¥: {e}")
        
        logger.info("ğŸ§ª æ‰€æœ‰å®¶åº­é¢„æ£€å®Œæˆï¼Œå¼€å§‹å¹¶è¡Œå…¨é‡è¿è¡Œã€‚\n")
        
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import threading
        
        # ä¸ºæ¯ä¸ªå®¶åº­åˆ›å»ºç‹¬ç«‹çš„æ—¥å¿—æ–‡ä»¶
        family_loggers = {}
        log_dir = output_dir / "family_logs"
        log_dir.mkdir(exist_ok=True)
        
        def setup_family_logger(family_id):
            """ä¸ºæ¯ä¸ªå®¶åº­åˆ›å»ºç‹¬ç«‹çš„ logger"""
            family_logger = logging.getLogger(f"family_{family_id}")
            family_logger.setLevel(logging.INFO)
            family_logger.handlers.clear()
            family_logger.propagate = False  # é˜²æ­¢æ—¥å¿—å‘ä¸Šä¼ æ’­åˆ°æ ¹logger
            
            # æ–‡ä»¶handler
            fh = logging.FileHandler(log_dir / f"{family_id}.log", encoding='utf-8', mode='w')
            fh.setLevel(logging.INFO)
            formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')
            fh.setFormatter(formatter)
            family_logger.addHandler(fh)
            
            # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°ï¼ˆå¸¦å®¶åº­IDå‰ç¼€ï¼‰
            ch = logging.StreamHandler()
            ch.setLevel(logging.INFO)
            ch_formatter = logging.Formatter(f'[{family_id}] %(message)s')
            ch.setFormatter(ch_formatter)
            family_logger.addHandler(ch)
            
            return family_logger
        
        for fam in families:
            family_loggers[fam] = setup_family_logger(fam)
        
        def process_family_with_logging(family_id):
            """å¸¦ç‹¬ç«‹æ—¥å¿—çš„å®¶åº­å¤„ç†å‡½æ•°"""
            family_logger = family_loggers[family_id]
            try:
                family_logger.info(f"â–¶ å¼€å§‹å¤„ç†å®¶åº­ {family_id}")
                
                # ç›´æ¥ä¼ å…¥family_loggerï¼Œé¿å…å†…éƒ¨å†åˆ›å»º
                process_family(
                    family_id,
                    output_dir,
                    split_ratio=args.split_ratio,
                    max_train_events=args.max_train_events,
                    max_test_samples=args.max_test_samples,
                    preflight=False,
                    family_logger=family_logger,
                )
                
                family_logger.info(f"âœ… å®Œæˆå®¶åº­ {family_id}")
                return family_id, "Success", None
            except Exception as e:
                family_logger.error(f"âŒ å®¶åº­ {family_id} å¤„ç†å¤±è´¥: {e}")
                import traceback
                family_logger.error(traceback.format_exc())
                return family_id, "Failed", str(e)
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰å®¶åº­
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {total} ä¸ªå®¶åº­...")
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=min(total, 3)) as executor:
            futures = {executor.submit(process_family_with_logging, fam): fam for fam in families}
            completed = 0
            
            for future in as_completed(futures):
                family_id, status, error = future.result()
                completed += 1
                summary[family_id] = status if status == "Success" else f"Failed: {error}"
                
                elapsed = time.time() - start_time
                logger.info(
                    f"ğŸ“Š è¿›åº¦: {completed}/{total} ({completed/total*100:.1f}%) | "
                    f"å·²å®Œæˆ: {family_id} ({status}) | æ€»è€—æ—¶: {elapsed:.1f}s"
                )
        
        total_elapsed = time.time() - start_time
        logger.info(f"\nğŸ‰ æ‰€æœ‰å®¶åº­å¤„ç†å®Œæˆï¼æ€»è€—æ—¶: {total_elapsed:.1f}s")
    
    logger.info(f"\nğŸ“Š ä»»åŠ¡æ€»ç»“: {summary}")

if __name__ == "__main__":
    main()
    
    logger.info("â³ ç­‰å¾…ç³»ç»Ÿåå°ä»»åŠ¡æ¸…ç† (çº¦10ç§’)...")
    time.sleep(10)
